{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da80993a",
   "metadata": {},
   "source": [
    "# PySpark 3.0中的新Pandas UDF和Python类型提示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04453bc4",
   "metadata": {},
   "source": [
    "> Pandas用户定义函数（UDF）是Apache SparkTM在数据科学方面最重要的增强之一。它们带来了许多好处，例如使用户能够使用PandasAPI并提高性能。\n",
    "\n",
    "然而，Pandas的UDF随着时间的推移而有机地发展，这导致了一些不一致，并在用户中造成了混乱。Apache Spark 3.0为Pandas UDF引入一个新的接口，该接口利用Python类型提示来解决`Pandas UDF`类型的激增问题，并帮助它们变得更加`Pythonic`和自我描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715b49a",
   "metadata": {},
   "source": [
    "## **Pandas UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817be125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26defaa6ce0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41237752",
   "metadata": {},
   "source": [
    "Pandas UDF是在Spark 2.3中引入的，请[参阅Pandas UDF for PySpark](https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)。Pandas为数据科学家所熟知，并与许多Python库和包（如[NumPy](https://numpy.org/)，[statsmodel](https://www.statsmodels.org/stable/index.html)和[scikit-learn）](https://scikit-learn.org/stable/)无缝集成，Pandas UDF不仅允许数据科学家扩展其工作负载，还可以利用Apache Spark中的Pandas API。\n",
    "\n",
    "用户定义的函数由以下人员执行：\n",
    "\n",
    "- [Apache Arrow](https://arrow.apache.org/)，在JVM和Python驱动程序/执行器之间直接交换数据，几乎零（反）序列化成本。\n",
    "- 函数内部的Pandas，用于Pandas实例和API。\n",
    "\n",
    "Pandas UDF与函数内部的Pandas API和Apache Arrow一起工作，用于交换数据。它允许向量化操作，与一次一行的Python UDF相比，可以将性能提高100倍。\n",
    "\n",
    "下面的示例显示了一个Pandas UDF，只需向每个值添加一个，其中它是用名为`pandas_plus_one`的函数定义的，该函数由指定为`pandas_udf`的Pandas UDF类型的`PandasUDFType.SCALAR`装饰。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5321c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                1.0|\n",
      "|                2.0|\n",
      "|                3.0|\n",
      "|                4.0|\n",
      "|                5.0|\n",
      "|                6.0|\n",
      "|                7.0|\n",
      "|                8.0|\n",
      "|                9.0|\n",
      "|               10.0|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    # `v` is a pandas Series\n",
    "    return v.add(1)  # outputs a pandas Series\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa399cc7",
   "metadata": {},
   "source": [
    "Python函数获取并输出一个Pandas Series。您可以通过使用此函数中丰富的Pandas API集来执行向量化操作，以便为每个值添加一个。序列化（反序列化）也可以通过在引擎盖下利用Apache Arrow自动矢量化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34a815",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## **Python类型提示**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c7e4",
   "metadata": {},
   "source": [
    "Python类型提示是在[PEP 484](https://www.python.org/dev/peps/pep-0484/)和Python 3.5中正式引入的。类型提示是Python中静态指示值类型的官方方法。请参见下面的示例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0f9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting(name: str) -> str:\n",
    "    return 'Hello ' + name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8982ae7",
   "metadata": {},
   "source": [
    "name：`str`表示name参数是str类型，`->`语法表示`greeting()`函数返回一个字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40bb5b",
   "metadata": {},
   "source": [
    "Python类型提示为PySpark和Pandas UDF上下文带来了两个显著的好处。\n",
    "\n",
    "- 它给出了函数应该做什么的明确定义，使用户更容易理解代码。例如，如果没有类型提示，用户就无法知道`greeting`是否可以接受`None`。它可以避免使用一堆测试用例记录这些微妙的案例和/或让用户自己测试和弄清楚的需要。\n",
    "- 它可以使执行静态分析更容易。像PyCharm和[Visual Studio Code](https://code.visualstudio.com/)这样的IDE可以利用类型注释来提供代码完成、显示错误，并支持更好的转到定义功能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a3c84",
   "metadata": {},
   "source": [
    "## **Pandas UDF类型的激增**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab96d3",
   "metadata": {},
   "source": [
    "自Apache Spark 2.3发布以来，已经实现了许多新的Pandas UDF，这使得用户很难了解新规范以及如何使用它们。例如，这里有三个输出几乎相同结果的Pandas UDF：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5f250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "@pandas_udf('long', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    # `v` is a pandas Series\n",
    "    return v + 1  # outputs a pandas Series\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0771644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "# New type of Pandas UDF in Spark 3.0.\n",
    "@pandas_udf('long', PandasUDFType.SCALAR_ITER)\n",
    "def pandas_plus_one(itr):\n",
    "    # `iterator` is an iterator of pandas Series.\n",
    "    return map(lambda v: v + 1, itr)  # outputs an iterator of pandas Series.\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4ceefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(\"id long\", PandasUDFType.GROUPED_MAP)\n",
    "def pandas_plus_one(pdf):\n",
    "    # `pdf` is a pandas DataFrame\n",
    "    return pdf + 1  # outputs a pandas DataFrame\n",
    "\n",
    "# `pandas_plus_one` can _only_ be used with `groupby(...).apply(...)`\n",
    "spark.range(10).groupby('id').apply(pandas_plus_one).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5080b6",
   "metadata": {},
   "source": [
    "尽管这些UDF类型中的每一种都有不同的用途，但有几种可以适用。在这个简单的例子中，你可以使用这三个中的任何一个。然而，每个Pandas UDF都期望不同的输入和输出类型，并以不同的方式工作，具有不同的语义和不同的性能。它让用户搞不清楚该使用和学习哪一个，以及每一个如何工作。\n",
    "\n",
    "此外，第一种和第二种情况下的`pandas_plus_one`可以在使用常规PySpark列的情况下使用。考虑`withColumn`的参数或具有其他表达式（如`pandas_plus_one(\"id\") + 1`）组合的函数。然而，最后的`pandas_plus_one`只能与`groupby(...).apply(pandas_plus_one)`一起使用。\n",
    "\n",
    "这种复杂程度引发了Spark开发人员的大量讨论，并推动了通过[官方提案](https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit?usp=sharing)引入带有Python类型提示的新Pandas API的努力。我们的目标是让用户能够使用Python类型提示自然地表达他们的pandas UDF，而不会像上面有问题的情况那样产生混乱。例如，上述情况可以写成如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15649a37",
   "metadata": {},
   "source": [
    "```python\n",
    "def pandas_plus_one(v: pd.Series) -> pd.Series:\n",
    "    return v + 1\n",
    "def pandas_plus_one(itr: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    return map(lambda v: v + 1, itr)\n",
    "def pandas_plus_one(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pdf + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86f57f",
   "metadata": {},
   "source": [
    "## **带有Python类型提示的新Pandas API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8e0f7",
   "metadata": {},
   "source": [
    "为了解决旧Pandas UDF的复杂性，从Apache Spark 3.0和Python 3.6及更高版本开始，可以使用Python类型提示（如`pandas.Series`、`pandas.DataFrame`、`Tuple`和`Iterator`）来表达新的Pandas UDF类型。\n",
    "\n",
    "此外，旧的Pandas UDF被分为两个API类别：Pandas UDF和Pandas Function API。虽然它们在内部以类似的方式工作，但存在明显的差异。\n",
    "\n",
    "您可以像使用其他PySpark列实例一样对待Pandas UDF。但是，您不能将Pandas Function API用于这些列实例。下面是这两个例子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219b5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|((pandas_plus_one((id - 1)) + LOG2(id)) + 1)|\n",
      "+--------------------------------------------+\n",
      "|                                        NULL|\n",
      "|                                         2.0|\n",
      "|                                         4.0|\n",
      "|                           5.584962500721156|\n",
      "|                                         7.0|\n",
      "|                           8.321928094887362|\n",
      "|                           9.584962500721156|\n",
      "|                          10.807354922057604|\n",
      "|                                        12.0|\n",
      "|                          13.169925001442312|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas UDF\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, log2, col\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "# pandas_plus_one(\"id\") is identically treated as _a SQL expression_ internally.\n",
    "# Namely, you can combine with other columns, functions and expressions.\n",
    "spark.range(10).select(\n",
    "    pandas_plus_one(col(\"id\") - 1) + log2(\"id\") + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac736ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas Function API\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pandas_plus_one(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    return map(lambda v: v + 1, iterator)\n",
    "\n",
    "\n",
    "# pandas_plus_one is just a regular Python function, and mapInPandas is\n",
    "# logically treated as _a separate SQL query plan_ instead of a SQL expression. \n",
    "# Therefore, direct interactions with other expressions are impossible.\n",
    "spark.range(10).mapInPandas(pandas_plus_one, schema=\"id long\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632eb69",
   "metadata": {},
   "source": [
    "另外，请注意，Pandas UDF需要Python类型提示，而Pandas Function API中的类型提示目前是可选的。Pandas Function API计划提供类型提示，将来可能会需要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de085b1",
   "metadata": {},
   "source": [
    "### **新Pandas UDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc5ce5",
   "metadata": {},
   "source": [
    "新的Pandas UDFs不是手动定义和指定每个Pandas UDF类型，而是从Python函数的给定Python类型提示中推断Pandas UDF类型。Pandas UDF中目前支持的Python类型提示有四种情况：\n",
    "\n",
    "- Series to Series 系列到系列\n",
    "- Iterator of Series to Iterator of Series 系列迭代器到系列迭代器\n",
    "- Iterator of Multiple Series to Iterator of Series 多重级数迭代器到级数迭代器\n",
    "- Series to Scalar (a single value) 系列到标量（单个值）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0cbcb",
   "metadata": {},
   "source": [
    "在我们深入研究每种情况之前，让我们看看使用新Pandas UDF的三个关键点。\n",
    "\n",
    "- 虽然Python类型提示在Python世界中通常是可选的，但为了使用新的Pandas UDF，必须为输入和输出指定Python类型提示。\n",
    "- 用户仍然可以通过手动指定`Pandas UDF`类型来使用旧方法。但是，鼓励使用Python类型提示。\n",
    "- 类型提示在所有情况下都应该使用`pandas.Series`。然而，有一个变体，其中`pandas.DataFrame`应该用于其输入或输出类型提示：当输入或输出列为`StructType.`时，请看下面的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b178e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+\n",
      "|long_col|string_col|       struct_col|\n",
      "+--------+----------+-----------------+\n",
      "|       1|  a string|{a nested string}|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1 string>\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb46145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|pandas_plus_len(long_col, string_col, struct_col)|\n",
      "+-------------------------------------------------+\n",
      "|                             {a nested string, 9}|\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def pandas_plus_len(\n",
    "        s1: pd.Series, s2: pd.Series, pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Regular columns are series and the struct column is a DataFrame.\n",
    "    pdf['col2'] = s1 + s2.str.len() \n",
    "    return pdf  # the struct column expects a DataFrame to return\n",
    "\n",
    "df.select(pandas_plus_len(\"long_col\", \"string_col\", \"struct_col\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e11da9",
   "metadata": {},
   "source": [
    "#### **系列到系列**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16321b5b",
   "metadata": {},
   "source": [
    "Series to Series映射到Apache Spark 2.3中引入的标量Pandas UDF。类型提示可以表示为`pandas.Series, ... -> pandas.Series`。它期望给定的函数接受一个或多个`pandas.Series`，并输出一个`pandas.Series`。输出长度应与输入长度相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351b601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518d345",
   "metadata": {},
   "source": [
    "上面的例子可以用标量Pandas UDF映射到旧的样式，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1079203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "\n",
    "@pandas_udf('long', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa007de",
   "metadata": {},
   "source": [
    "#### **系列迭代器到系列迭代器**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36159d11",
   "metadata": {},
   "source": [
    "这是Apache Spark 3.0中的一种新型Pandas UDF。它是Series to Series的变体，类型提示可以表示为`Iterator[pd.Series] -> Iterator[pd.Series]`。该函数接受并输出一个迭代器`pandas.Series`。\n",
    "\n",
    "整个输出的长度必须与整个输入的长度相同。因此，只要整个输入和输出的长度相同，它就可以从输入迭代器中预取数据。给定的函数应该接受一列作为输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2359bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    return map(lambda s: s + 1, iterator)\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006b4a5",
   "metadata": {},
   "source": [
    "当UDF执行需要昂贵的初始化某些状态时，它也很有用。下面的伪代码说明了这种情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e3a67",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "@pandas_udf(\"long\")\n",
    "def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Do some expensive initialization with a state\n",
    "    state = very_expensive_initialization()\n",
    "    for x in iterator:\n",
    "        # Use that state for the whole iterator.\n",
    "        yield calculate_with_state(x, state)\n",
    "\n",
    "df.select(calculate(\"value\")).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a7543",
   "metadata": {},
   "source": [
    "Iterator of Series到Iterator of Series也可以映射到旧的Pandas UDF样式。请参见下面的示例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7535566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType                                                                                                                                                                                                                   \n",
    "\n",
    "@pandas_udf('long', PandasUDFType.SCALAR_ITER)\n",
    "def pandas_plus_one(iterator):\n",
    "    return map(lambda s: s + 1, iterator)\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14881ea",
   "metadata": {},
   "source": [
    "#### **多重级数迭代器到级数迭代器**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38caf8b3",
   "metadata": {},
   "source": [
    "这种类型的Pandas UDF也将在Apache Spark 3.0中引入，以及Iterator of Series to Iterator of Series。类型提示可以表示为`Iterator[Tuple[pandas.Series, ...]] -> Iterator[pandas.Series]`。\n",
    "\n",
    "它具有与系列迭代器相似的特性和限制。给定的函数接受一个元组`pandas.Series`的迭代器，并输出一个迭代器`pandas.Series`。在使用某些状态和预取输入数据时，它也很有用。整个输出的长度也应该与整个输入的长度相同。但是，给定的函数应该接受多个列作为输入，这与Iterator of Series to Iterator of Series不同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3767d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|multiply_two(id, id)|\n",
      "+--------------------+\n",
      "|                   0|\n",
      "|                   1|\n",
      "|                   4|\n",
      "|                   9|\n",
      "|                  16|\n",
      "|                  25|\n",
      "|                  36|\n",
      "|                  49|\n",
      "|                  64|\n",
      "|                  81|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def multiply_two(\n",
    "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    return (a * b for a, b in iterator)\n",
    "\n",
    "spark.range(10).select(multiply_two(\"id\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83a383",
   "metadata": {},
   "source": [
    "这也可以映射到旧的Pandas UDF样式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ec56d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|multiply_two(id, id)|\n",
      "+--------------------+\n",
      "|                   0|\n",
      "|                   1|\n",
      "|                   4|\n",
      "|                   9|\n",
      "|                  16|\n",
      "|                  25|\n",
      "|                  36|\n",
      "|                  49|\n",
      "|                  64|\n",
      "|                  81|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "@pandas_udf('long', PandasUDFType.SCALAR_ITER)\n",
    "def multiply_two(iterator):\n",
    "    return (a * b for a, b in iterator)\n",
    "\n",
    "spark.range(10).select(multiply_two(\"id\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40b8c9",
   "metadata": {},
   "source": [
    "#### **系列到标量**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fccea",
   "metadata": {},
   "source": [
    "Series to Scalar映射到Apache Spark 2.4中引入的分组聚合Pandas UDF。类型提示表示为`pandas.Series, ... -> Any`。该函数接受一个或多个panda.Series并输出一个原始数据类型。返回的标量可以是Python原语类型，例如，`int`、`float`或NumPy数据类型（如`numpy.int64`、`numpy.float64`等）。`Any`应相应地理想地为特定标量类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74692b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pandas_mean(v)|\n",
      "+--------------+\n",
      "|          21.0|\n",
      "+--------------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|pandas_mean(v)|\n",
      "+---+--------------+\n",
      "|  1|           3.0|\n",
      "|  2|          18.0|\n",
      "+---+--------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|pandas_mean(v) OVER (PARTITION BY id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|                                                                                           3.0|\n",
      "|                                                                                           3.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def pandas_mean(v: pd.Series) -> float:\n",
    "    return v.sum()\n",
    "\n",
    "df.select(pandas_mean(df['v'])).show()\n",
    "df.groupby(\"id\").agg(pandas_mean(df['v'])).show()\n",
    "df.select(pandas_mean(df['v']).over(Window.partitionBy('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7763cc",
   "metadata": {},
   "source": [
    "上面的示例可以转换为具有分组聚合Pandas UDF的示例，如您所见：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2e94de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pandas_mean(v)|\n",
      "+--------------+\n",
      "|          21.0|\n",
      "+--------------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|pandas_mean(v)|\n",
      "+---+--------------+\n",
      "|  1|           3.0|\n",
      "|  2|          18.0|\n",
      "+---+--------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|pandas_mean(v) OVER (PARTITION BY id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|                                                                                           3.0|\n",
      "|                                                                                           3.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def pandas_mean(v):\n",
    "    return v.sum()\n",
    "\n",
    "df.select(pandas_mean(df['v'])).show()\n",
    "df.groupby(\"id\").agg(pandas_mean(df['v'])).show()\n",
    "df.select(pandas_mean(df['v']).over(Window.partitionBy('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16149484",
   "metadata": {},
   "source": [
    "### **新的Pandas函数API**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdd080",
   "metadata": {},
   "source": [
    "Apache Spark 3.0中的这个新类别使您能够直接应用Python原生函数，该函数针对PySpark DataFrame获取并输出Pandas实例。Apache Spark 3.0中支持的Pandas Functions API包括：grouped map、map和co—grouped map。\n",
    "\n",
    "请注意，分组映射Pandas UDF现在被归类为组映射Pandas Function API。如前所述，Pandas Function API中的Python类型提示目前是可选的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f55532",
   "metadata": {},
   "source": [
    "#### **分组地图**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4281101",
   "metadata": {},
   "source": [
    "Pandas Function API中的Grouped map在分组的DataFrame中是`applyInPandas`，例如，`df.groupby(...)`.这被映射到旧Pandas UDF类型中的分组映射Pandas UDF。它将每个组映射到函数中的每个`pandas.DataFrame`。注意，它不要求输出与输入的长度相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd1b3f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").applyInPandas(subtract_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b28ae",
   "metadata": {},
   "source": [
    "Grouped map type映射到Spark 2.3支持的Grouped map Pandas UDF，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "825e78a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdaaf7",
   "metadata": {},
   "source": [
    "#### **地图**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5c0aa",
   "metadata": {},
   "source": [
    "Map Pandas Function API是DataFrame中的`mapInPandas`。这是Apache Spark 3.0的新特性。它映射每个分区中的每个批次并转换每个批次。该函数接受一个迭代器`pandas.DataFrame`并输出一个迭代器`pandas.DataFrame`。输出长度不需要与输入大小匹配。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24a59fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "\n",
    "def pandas_filter(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "df.mapInPandas(pandas_filter, schema=df.schema).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df902400",
   "metadata": {},
   "source": [
    "#### **联合分组地图**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b790324",
   "metadata": {},
   "source": [
    "Co-grouped map，在Co-grouped DataFrame（如`applyInPandas`）中的`df.groupby(...).cogroup(df.groupby(...))`，也将在Apache Spark 3.0中引入。与分组映射类似，它将每个组映射到函数中的每个`pandas.DataFrame`，但它通过公共键与另一个DataFrame分组，然后将函数应用于每个cogroup。同样，对输出长度也没有限制。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2cbfe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+\n",
      "|time| id| v1| v2|\n",
      "+----+---+---+---+\n",
      "|1201|  1|1.0|  x|\n",
      "|1202|  1|3.0|  x|\n",
      "|1201|  2|2.0|  y|\n",
      "|1202|  2|4.0|  y|\n",
      "+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = spark.createDataFrame(\n",
    "    [(1201, 1, 1.0), (1201, 2, 2.0), (1202, 1, 3.0), (1202, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "df2 = spark.createDataFrame(\n",
    "    [(1201, 1, \"x\"), (1201, 2, \"y\")], (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "def asof_join(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_asof(left, right, on=\"time\", by=\"id\")\n",
    "\n",
    "df1.groupby(\"id\").cogroup(\n",
    "    df2.groupby(\"id\")\n",
    ").applyInPandas(asof_join, \"time int, id int, v1 double, v2 string\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e175d6",
   "metadata": {},
   "source": [
    "## **结论和今后的工作**\n",
    "\n",
    "即将发布的Apache Spark 3.0（[阅读我们的预览博客了解详细信息](https://www.databricks.com/blog/2020/05/13/now-on-databricks-a-technical-preview-of-databricks-runtime-7-including-a-preview-of-apache-spark-3-0.html)）。将提供Python类型提示，使用户更容易表达Pandas UDF和Pandas Function API。将来，我们应该考虑在Pandas UDF和Pandas Function API中添加对其他类型提示组合的支持。目前，支持的情况只是Python类型提示的许多可能组合中的一小部分。Apache Spark社区中还有其他正在进行的讨论。访问[边讨论和未来](https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit#heading=h.h3ncjpk6ujqu)改进了解更多信息。\n",
    "\n",
    "在我们的预览网络研讨会中了解有关Spark 3.0的更多[信息。](https://www.databricks.com/p/webinar/apache-spark-3-0) 作为Databricks 7.0 Beta的一部分，立即[在Databricks上免费](https://www.databricks.com/try-databricks)试用这些新功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e6d36",
   "metadata": {},
   "source": [
    "https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
