{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f225a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ecd8210070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea3258",
   "metadata": {},
   "source": [
    "# pandas-udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6970ed",
   "metadata": {},
   "source": [
    "> Pandas UDF (User-Defined Function) 是 PySpark 中的一个功能,它允许您在 Spark 中使用 Pandas 函数来进行数据处理和转换。这种功能可以提高代码的可读性和开发效率,因为 Pandas 提供了丰富的数据处理函数和方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc30680",
   "metadata": {},
   "source": [
    "**Pandas UDF 有以下几种使用方式:**\n",
    "\n",
    "* 标量 Pandas UDF:\n",
    "    - 接受单个输入列,并返回单个输出列。\n",
    "    - 可以使用 `@pandas_udf(..., PandasUDFType.SCALAR)` 装饰器定义。\n",
    "* Grouped Map Pandas UDF:\n",
    "    - 接受一个 `Pandas DataFrame` 作为输入,并返回一个 `Pandas DataFrame`。\n",
    "    - 可以使用 `@pandas_udf(..., PandasUDFType.GROUPED_MAP)` 装饰器定义。\n",
    "* Grouped Aggregate Pandas UDF:\n",
    "    - 接受一个 Pandas DataFrame 作为输入,并返回一个 Pandas Series。\n",
    "    - 可以使用 `@pandas_udf(..., PandasUDFType.GROUPED_AGG)` 装饰器定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ecf7f",
   "metadata": {},
   "source": [
    "## 标量 Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df92170",
   "metadata": {},
   "source": [
    "下面是一个简单的示例,展示如何使用标量 Pandas UDF 计算列的平方:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e468df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(returnType=\"double\", functionType=PandasUDFType.SCALAR)\n",
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebcc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([1, 2, 3, 4, 5], \"int\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5d46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|value|squared|\n",
      "+-----+-------+\n",
      "|    1|    1.0|\n",
      "|    2|    4.0|\n",
      "|    3|    9.0|\n",
      "|    4|   16.0|\n",
      "|    5|   25.0|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"squared\", square(df[0]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a812d8",
   "metadata": {},
   "source": [
    "这个示例演示了如何定义一个名为 square 的标量 Pandas UDF,它接受一个输入列并返回该列的平方值。然后,我们将这个 UDF 应用到一个 Spark DataFrame 上,创建了一个新的 squared 列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5f2ed",
   "metadata": {},
   "source": [
    "## Grouped Map Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875606ae",
   "metadata": {},
   "source": [
    "> Grouped Map Pandas UDF 是 Pandas UDF 中的一种重要类型,它允许您在 Spark 中对分组数据应用 Pandas 函数。这种 UDF **接受一个 Pandas DataFrame 作为输入,并返回一个 Pandas DataFrame。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7b4d8",
   "metadata": {},
   "source": [
    "**以下是 Grouped Map Pandas UDF 的一些主要特点:**\n",
    "\n",
    "- **分组处理**: Grouped Map Pandas UDF 能够对 Spark DataFrame 中的分组数据应用自定义的 Pandas 函数。这种方式可以利用 Pandas 丰富的数据处理能力,同时保持 Spark 的分布式计算优势。\n",
    "- **灵活性**: 与标量 Pandas UDF 只能处理单个列不同,Grouped Map Pandas UDF 可以处理整个 Pandas DataFrame,从而提供更大的灵活性和表达能力。\n",
    "- **返回 Pandas DataFrame**: Grouped Map Pandas UDF 的输出是一个 Pandas DataFrame,这使得后续的数据处理和转换更加方便。\n",
    "\n",
    "下面是一个示例,演示如何使用 Grouped Map Pandas UDF 计算每个分组的平均值和标准差:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c2f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06142a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"mean\", DoubleType(), True),\n",
    "    StructField(\"std\", DoubleType(), True), \n",
    "])\n",
    "\n",
    "@pandas_udf(returnType=schema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "def calc_stats(pdf: pd.DataFrame):\n",
    "    mean = pdf.mean()\n",
    "    std = pdf.std()\n",
    "    return pd.DataFrame({'mean': mean, 'std': std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b154585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|value|\n",
      "+-----+-----+\n",
      "|    1|   10|\n",
      "|    1|   20|\n",
      "|    1|   30|\n",
      "|    2|    5|\n",
      "|    2|   15|\n",
      "|    2|   25|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 10), (1, 20), (1, 30),\n",
    "    (2, 5), (2, 15), (2, 25)\n",
    "], [\"group\", \"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e15bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|mean| std|\n",
      "+----+----+\n",
      "| 1.0| 0.0|\n",
      "|20.0|10.0|\n",
      "| 2.0| 0.0|\n",
      "|15.0|10.0|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df.groupBy(\"group\").apply(calc_stats)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fb57b",
   "metadata": {},
   "source": [
    "在这个示例中:\n",
    "\n",
    "* 我们定义了一个名为 calc_stats 的 Grouped Map Pandas UDF,它接受一个 Pandas DataFrame 作为输入,并返回一个包含 mean 和 std 列的 Pandas DataFrame。\n",
    "* 我们创建了一个包含 group 和 value 列的 Spark DataFrame。\n",
    "* 使用 `groupBy(\"group\").apply(calc_stats)` 对数据进行分组并应用 calc_stats UDF。\n",
    "* 最终结果是一个新的 Spark DataFrame,其中包含每个分组的平均值和标准差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea29d1",
   "metadata": {},
   "source": [
    "Spark 正在逐步弃用旧版本的 Pandas UDF API,并推荐使用新的 applyInPandas API。applyInPandas 提供了与旧版 Pandas UDF 类似的功能,但更加灵活和易用。\n",
    "\n",
    "下面是一个示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61c07ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e045c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(pdf):\n",
    "    mean = pdf[\"value\"].mean()\n",
    "    std = pdf[\"value\"].std()\n",
    "    return pdf.assign(mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1607269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|value|\n",
      "+-----+-----+\n",
      "|    1|   10|\n",
      "|    1|   20|\n",
      "|    1|   30|\n",
      "|    2|    5|\n",
      "|    2|   15|\n",
      "|    2|   25|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 10), (1, 20), (1, 30),\n",
    "    (2, 5), (2, 15), (2, 25)\n",
    "], [\"group\", \"value\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61b8a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+\n",
      "|group|value|mean| std|\n",
      "+-----+-----+----+----+\n",
      "|    1|   10|20.0|10.0|\n",
      "|    1|   20|20.0|10.0|\n",
      "|    1|   30|20.0|10.0|\n",
      "|    2|    5|15.0|10.0|\n",
      "|    2|   15|15.0|10.0|\n",
      "|    2|   25|15.0|10.0|\n",
      "+-----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df.groupBy(\"group\").applyInPandas(\n",
    "    calc_stats, schema=StructType([\n",
    "    StructField(\"group\", IntegerType()),\n",
    "    StructField(\"value\", IntegerType()),\n",
    "    StructField(\"mean\", DoubleType()),\n",
    "    StructField(\"std\", DoubleType())\n",
    "]))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05358f8c",
   "metadata": {},
   "source": [
    "* 在这个更新的版本中:\n",
    "\n",
    "* 我们定义了一个名为 calc_stats 的函数,它接受一个 Pandas DataFrame 作为输入,并返回一个新的 Pandas DataFrame,其中包含原始 DataFrame 以及计算的平均值和标准差。\n",
    "* 我们使用 applyInPandas 方法将 calc_stats 函数应用到分组的 Spark DataFrame 上。\n",
    "* 我们还定义了输出 Spark DataFrame 的架构(schema),包括原始的 group 和 value 列以及新的 mean 和 std 列。\n",
    "* 通过使用 applyInPandas 方法,您可以避免旧版 Pandas UDF API 的警告,并且未来升级 Spark 时,您的代码也会更加稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab24cc7",
   "metadata": {},
   "source": [
    "## Grouped Aggregate Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76825f",
   "metadata": {},
   "source": [
    "`Grouped Aggregate Pandas UDF` 是 PySpark 中一种特殊类型的用户自定义函数(UDF),它允许在 PySpark DataFrame 的分组上应用 Pandas 函数,并将结果聚合为一个新的 PySpark DataFrame。\n",
    "\n",
    "与普通的 applyInPandas 不同,Grouped Aggregate Pandas UDF 更加专注于聚合计算,并提供了更高效的实现。\n",
    "\n",
    "Grouped Aggregate Pandas UDF 的基本用法如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfb453c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(DoubleType(), PandasUDFType.GROUPED_AGG)\n",
    "def mean_udf(pdf):\n",
    "    return pdf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4880c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|value|\n",
      "+-----+-----+\n",
      "|    1|   10|\n",
      "|    1|   20|\n",
      "|    1|   30|\n",
      "|    2|    5|\n",
      "|    2|   15|\n",
      "|    2|   25|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 10), (1, 20), (1, 30),\n",
    "    (2, 5), (2, 15), (2, 25)\n",
    "], [\"group\", \"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7249715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|group|mean_udf(value)|\n",
      "+-----+---------------+\n",
      "|    1|           20.0|\n",
      "|    2|           15.0|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"group\").agg(mean_udf(F.col(\"value\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526fb26",
   "metadata": {},
   "source": [
    "其中:\n",
    "\n",
    "- `@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)` 是一个装饰器,用于定义 `Grouped Aggregate Pandas UDF`\n",
    "- `mean_udf` 是自定义的 `Pandas` 函数,它接收一个 `Pandas DataFrame pdf` 并返回平均值\n",
    "- `original_df.groupBy(\"some_column\").agg(mean_udf(\"value_column\"))` 是将 mean_udf 应用到 PySpark DataFrame 的分组上\n",
    "\n",
    "Grouped Aggregate Pandas UDF 的主要特点包括:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931fd43",
   "metadata": {},
   "source": [
    "- 高效的实现: Grouped Aggregate Pandas UDF 比普通的 applyInPandas 更加高效,因为它利用了 PySpark 的分组聚合机制,减少了数据传输和计算开销。\n",
    "- 支持多种聚合函数: 你可以定义各种聚合函数,如 sum、count、std 等,并应用到 PySpark DataFrame 的分组上。\n",
    "- 与其他 PySpark 函数兼容: Grouped Aggregate Pandas UDF 可以与 PySpark 的其他聚合函数(agg、groupBy等)一起使用,增强了灵活性。\n",
    "- 保留 PySpark 的优势: 使用 Grouped Aggregate Pandas UDF 之后,结果仍然是 PySpark DataFrame,你可以继续使用 PySpark 的其他功能。\n",
    "    \n",
    "下面是另一个具体的例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ad7fd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pandas_mean(v)|\n",
      "+--------------+\n",
      "|          21.0|\n",
      "+--------------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|pandas_mean(v)|\n",
      "+---+--------------+\n",
      "|  1|           3.0|\n",
      "|  2|          18.0|\n",
      "+---+--------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|pandas_mean(v) OVER (PARTITION BY id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|                                                                                           3.0|\n",
      "|                                                                                           3.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def pandas_mean(v):\n",
    "    return v.sum()\n",
    "\n",
    "df.select(pandas_mean(df['v'])).show()\n",
    "df.groupby(\"id\").agg(pandas_mean(df['v'])).show()\n",
    "df.select(pandas_mean(df['v']).over(Window.partitionBy('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4400f",
   "metadata": {},
   "source": [
    "- [New Pandas UDFs and Python Type Hints in the Upcoming Release of Apache Spark 3.0](https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)\n",
    "- [sql pyspark pandas with arrow](https://spark.apache.org/docs/2.4.0/sql-pyspark-pandas-with-arrow.html)\n",
    "- [sql pyspark pandas with arrow](https://spark.apache.org/docs/2.4.0/sql-pyspark-pandas-with-arrow.html)\n",
    "\n",
    "\n",
    "- [sql pyspark pandas with arrow](https://spark.apache.org/docs/2.4.0/sql-pyspark-pandas-with-arrow.html)\n",
    "- [dineshdharme](https://gist.github.com/dineshdharme/d0247100dd0b29034e5bc46bc883504d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d325a",
   "metadata": {},
   "source": [
    "创建一个pandas用户定义的函数（也称为向量化的用户定义函数）。\n",
    "\n",
    "Pandas UDF是由Spark使用Arrow执行的用户定义函数， data和Pandas来处理数据，这允许矢量化操作。一个Pandas UDF 使用pandas_udf作为装饰器或包装函数来定义，而没有 需要额外的配置。Pandas UDF的行为与常规PySpark函数相同 API一般。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640017d",
   "metadata": {},
   "source": [
    "## 为了使用此API，通常导入以下内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b09f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15705c4a",
   "metadata": {},
   "source": [
    "> 从Spark 3.0和Python 3.6+，Python类型提示 检测功能类型如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07a90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(IntegerType())\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5938b",
   "metadata": {},
   "source": [
    "> 在Spark 3.0之前，pandas UDF使用functionType来决定执行类型，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43dd9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def slen(s):\n",
    "    return s.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247aa9c1",
   "metadata": {},
   "source": [
    "最好为pandas UDF指定类型提示，而不是指定pandas UDF 类型通过functionType，这将在未来版本中弃用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743f331",
   "metadata": {},
   "source": [
    "请注意，类型提示在所有情况下都应该使用pandas.Series，但有一个变体 当输入类型提示为 或输出列为pyspark.sql.types.StructType。以下示例显示 一个Pandas UDF，它接受长列、字符串列和结构列，并输出一个结构 柱它要求函数指定pandas.Series和 pandas.DataFrame如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c6de68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x229cfcaa1d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ef5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f1ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67705259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a804b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|to_upper(name)|\n",
      "+--------------+\n",
      "|      JOHN DOE|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(to_upper(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55226d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|split_expand(name)|\n",
      "+------------------+\n",
      "|       {John, Doe}|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"first string, last string\")\n",
    "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
    "    return s.str.split(expand=True)\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(split_expand(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d244ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a860af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b451107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    center = np.mean(vectors, axis=0)\n",
    "    distances = np.linalg.norm(vectors - center,axis=1) ** 2\n",
    "    return float(np.sum(distances) / len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94cf2efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.69126354384197"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost(np.random.random((10, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea10b062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(returnType=FloatType(), functionType=PandasUDFType.GROUPED_AGG)\n",
    "def calculate_cost(vectors):\n",
    "    import numpy as np\n",
    "    vectors = np.array(vectors.tolist())\n",
    "    center = np.mean(vectors, axis=0)\n",
    "    distances = np.linalg.norm(vectors - center,axis=1) ** 2\n",
    "    return float(np.sum(distances) / len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71a148d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_=spark.createDataFrame([\n",
    "    (1, [1.0,2.0,3.0]),\n",
    "    (1, [4.0,5.0,6.0]),\n",
    "    (1, [7.0,8.0,9.0]),\n",
    "    (2, [1.0,2.0,3.0]),\n",
    "    (2, [4.0,5.0,6.0]),\n",
    "    (2, [7.0,8.0,19.0]),\n",
    "], [\"k\",\"vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ae6125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((10, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e6d888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(\n",
    "        vector1: np.array,\n",
    "        vector2: np.array,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "\n",
    "    :param vector1:\n",
    "    :param vector2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1, axis=1, keepdims=True)\n",
    "    norm_vector2 = np.linalg.norm(vector2, axis=0, keepdims=True)\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecaf35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.mean(data, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57b3095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7d26877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 1024)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((vectors[:, np.newaxis, :] - vectors[np.newaxis, :, :]) ** 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf5f1b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((vectors[:, np.newaxis, :] - vectors[np.newaxis, :, :]) ** 2, axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bee90559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 1024)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[np.newaxis, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43a6db5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 13.27363914, 13.28169283, 13.08598339, 12.63868528,\n",
       "        12.87749279, 13.11084234, 12.9612079 , 13.22347233, 12.94993754],\n",
       "       [13.27363914,  0.        , 13.17356742, 13.05944711, 13.51668028,\n",
       "        13.58946275, 13.44044147, 13.52471714, 13.39231052, 13.06377062],\n",
       "       [13.28169283, 13.17356742,  0.        , 12.9471896 , 12.69430728,\n",
       "        13.19872245, 13.37827878, 13.08627054, 13.35102986, 12.80680542],\n",
       "       [13.08598339, 13.05944711, 12.9471896 ,  0.        , 12.9950665 ,\n",
       "        13.19778614, 12.57414198, 12.98743466, 12.93069745, 12.9850471 ],\n",
       "       [12.63868528, 13.51668028, 12.69430728, 12.9950665 ,  0.        ,\n",
       "        12.96493448, 13.35321711, 13.02346397, 13.24807683, 12.90623346],\n",
       "       [12.87749279, 13.58946275, 13.19872245, 13.19778614, 12.96493448,\n",
       "         0.        , 13.34908547, 13.35354952, 13.48634552, 12.65401174],\n",
       "       [13.11084234, 13.44044147, 13.37827878, 12.57414198, 13.35321711,\n",
       "        13.34908547,  0.        , 13.04400699, 13.00245814, 13.05835017],\n",
       "       [12.9612079 , 13.52471714, 13.08627054, 12.98743466, 13.02346397,\n",
       "        13.35354952, 13.04400699,  0.        , 12.98337879, 12.75019416],\n",
       "       [13.22347233, 13.39231052, 13.35102986, 12.93069745, 13.24807683,\n",
       "        13.48634552, 13.00245814, 12.98337879,  0.        , 12.71269136],\n",
       "       [12.94993754, 13.06377062, 12.80680542, 12.9850471 , 12.90623346,\n",
       "        12.65401174, 13.05835017, 12.75019416, 12.71269136,  0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "563fa8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.589462751942932"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "46004f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.589462751942932"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc4ec946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.28169283, 13.17356742,  0.        , 12.9471896 , 12.69430728,\n",
       "       13.19872245, 13.37827878, 13.08627054, 13.35102986, 12.80680542])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5fc7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f44ae8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "09ef5f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 6, 5, 1, 1, 1, 1, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(distances, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f7c0ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 6, 5, 1, 1, 1, 1, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(distances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea2d2f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1024), (1, 1024))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e585fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88242862, 0.87616786, 0.88299337, 0.88012671, 0.88252907,\n",
       "       0.87912349, 0.87579674, 0.88558641, 0.87809565, 0.88416553])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(data, center.T).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "464e7d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(cosine_similarity(data, center.T).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "030273fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "|  k|          vector|\n",
      "+---+----------------+\n",
      "|  1| [1.0, 2.0, 3.0]|\n",
      "|  1| [4.0, 5.0, 6.0]|\n",
      "|  1| [7.0, 8.0, 9.0]|\n",
      "|  2| [1.0, 2.0, 3.0]|\n",
      "|  2| [4.0, 5.0, 6.0]|\n",
      "|  2|[7.0, 8.0, 19.0]|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clusters_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a4336d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+\n",
      "|  k|calculate_cost(vector)|\n",
      "+---+----------------------+\n",
      "|  1|                  18.0|\n",
      "|  2|              60.22222|\n",
      "+---+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clusters_=df_clusters_.groupBy(\"k\").agg(calculate_cost(F.col(\"vector\")))\n",
    "df_clusters_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3d556",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
