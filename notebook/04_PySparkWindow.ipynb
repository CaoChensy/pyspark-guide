{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! https://zhuanlan.zhihu.com/p/446607001\n",
    "\n",
    "![Image](https://pic4.zhimg.com/80/v2-5db1a82996ec388725185ae900a58008.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【PySpark】窗口函数Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PySpark Window` 函数用于计算输入行范围内的结果，例如排名、行号等。在本文中，我解释了窗口函数的概念、语法，最后解释了如何将它们与 `PySpark SQL` 和 `PySpark DataFrame API` 一起使用。当我们需要在 `DataFrame` 列的特定窗口中进行聚合操作时，这些会派上用场。`Window`函数在实际业务场景中非常实用，用的好的话能避免很多浪费时间的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window函数分类为三种：\n",
    "- 排名函数 `ranking functions`包括:\n",
    "    - row_number()\n",
    "    - rank()\n",
    "    - dense_rank()\n",
    "    - percent_rank()\n",
    "    - ntile()\n",
    "- 解析函数 `analytic functions`包括:\n",
    "    - cume_dist()\n",
    "    - lag()\n",
    "    - lead()\n",
    "- 聚合函数 `aggregate functions`包括:\n",
    "    - sum()\n",
    "    - first()\n",
    "    - last()\n",
    "    - max()\n",
    "    - min()\n",
    "    - mean()\n",
    "    - stddev()\n",
    "\n",
    "下面依次详解上述三类函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T03:41:43.718743Z",
     "start_time": "2021-06-30T03:41:43.639156Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 创建一个 PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T03:40:14.625684Z",
     "start_time": "2021-06-30T03:39:51.958138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|name   |department|salary|\n",
      "+-------+----------+------+\n",
      "|Ali    |Sales     |8000  |\n",
      "|Bob    |Sales     |7000  |\n",
      "|Cindy  |Sales     |7500  |\n",
      "|Davd   |Finance   |10000 |\n",
      "|Elena  |Sales     |8000  |\n",
      "|Fancy  |Finance   |12000 |\n",
      "|George |Finance   |11000 |\n",
      "|Haffman|Marketing |7000  |\n",
      "|Ilaja  |Marketing |8000  |\n",
      "|Joey   |Sales     |9000  |\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary = [\n",
    "    (\"Ali\", \"Sales\", 8000),\n",
    "    (\"Bob\", \"Sales\", 7000),\n",
    "    (\"Cindy\", \"Sales\", 7500),\n",
    "    (\"Davd\", \"Finance\", 10000),\n",
    "    (\"Elena\", \"Sales\", 8000),\n",
    "    (\"Fancy\", \"Finance\", 12000),\n",
    "    (\"George\", \"Finance\", 11000),\n",
    "    (\"Haffman\", \"Marketing\", 7000),\n",
    "    (\"Ilaja\", \"Marketing\", 8000),\n",
    "    (\"Joey\", \"Sales\", 9000)]\n",
    " \n",
    "columns= [\"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = employee_salary, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 窗口函数 `ranking functions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 `row_number()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`row_number()` 窗口函数用于给出从1开始到每个窗口分区的结果的连续行号。\n",
    "\n",
    "与 `groupBy` 不同 `Window` 以 `partitionBy` 作为分组条件，`orderBy` 对 `Window` 分组内的数据进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T03:50:34.290855Z",
     "start_time": "2021-06-30T03:50:20.945355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | name    | department   |   salary |   row_number |\n",
      "|---:|:--------|:-------------|---------:|-------------:|\n",
      "|  0 | Bob     | Sales        |     7000 |            1 |\n",
      "|  1 | Cindy   | Sales        |     7500 |            2 |\n",
      "|  2 | Ali     | Sales        |     8000 |            3 |\n",
      "|  3 | Elena   | Sales        |     8000 |            4 |\n",
      "|  4 | Joey    | Sales        |     9000 |            5 |\n",
      "|  5 | Davd    | Finance      |    10000 |            1 |\n",
      "|  6 | George  | Finance      |    11000 |            2 |\n",
      "|  7 | Fancy   | Finance      |    12000 |            3 |\n",
      "|  8 | Haffman | Marketing    |     7000 |            1 |\n",
      "|  9 | Ilaja   | Marketing    |     8000 |            2 |\n"
     ]
    }
   ],
   "source": [
    "# 以 department 字段进行分组，以 salary 倒序排序\n",
    "# 按照部门对薪水排名，薪水最低的为第一名\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.asc(\"salary\"))\n",
    "# 分组内增加 row_number\n",
    "df_part = df.withColumn(\n",
    "    \"row_number\", \n",
    "    F.row_number().over(windowSpec)\n",
    ")\n",
    "print(df_part.toPandas().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上面的数据，发现同样的薪水会有不同的排名（都是$8000$的薪水，有的第二有的第三），这是因为`row_number()`是按照行来给定序号，其不关注实际数值的大小。由此我们可以引申出另一个用于给出排序数的函数$rank$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用场景\n",
    "\n",
    "- 选取本部门工资收入第$N$高的记录\n",
    "- （思考）选取某日第$N$笔交易记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T03:52:34.163622Z",
     "start_time": "2021-06-30T03:52:20.209894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | name   | department   |   salary |   row_number |\n",
      "|---:|:-------|:-------------|---------:|-------------:|\n",
      "|  0 | Cindy  | Sales        |     7500 |            2 |\n",
      "|  1 | George | Finance      |    11000 |            2 |\n",
      "|  2 | Ilaja  | Marketing    |     8000 |            2 |\n"
     ]
    }
   ],
   "source": [
    "print(df_part.where(F.col('row_number') == 2).toPandas().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rank()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rank()`用来给按照指定列排序的分组窗增加一个排序的序号，\n",
    "如果有相同数值，则排序数相同，下一个序数顺延一位。来看如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | name    | department   |   salary |   rank |\n",
      "|---:|:--------|:-------------|---------:|-------:|\n",
      "|  0 | Joey    | Sales        |     9000 |      1 |\n",
      "|  1 | Ali     | Sales        |     8000 |      2 |\n",
      "|  2 | Elena   | Sales        |     8000 |      2 |\n",
      "|  3 | Cindy   | Sales        |     7500 |      4 |\n",
      "|  4 | Bob     | Sales        |     7000 |      5 |\n",
      "|  5 | Fancy   | Finance      |    12000 |      1 |\n",
      "|  6 | George  | Finance      |    11000 |      2 |\n",
      "|  7 | Davd    | Finance      |    10000 |      3 |\n",
      "|  8 | Ilaja   | Marketing    |     8000 |      1 |\n",
      "|  9 | Haffman | Marketing    |     7000 |      2 |\n"
     ]
    }
   ],
   "source": [
    "# 使用 rank 排序，都是8000的薪水，就同列第二\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df_rank = df.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "print(df_rank.toPandas().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | name    | department   |   salary |   rank |\n",
      "|---:|:--------|:-------------|---------:|-------:|\n",
      "|  0 | Ali     | Sales        |     8000 |      2 |\n",
      "|  1 | Elena   | Sales        |     8000 |      2 |\n",
      "|  2 | George  | Finance      |    11000 |      2 |\n",
      "|  3 | Haffman | Marketing    |     7000 |      2 |\n"
     ]
    }
   ],
   "source": [
    "print(df_rank.where(F.col(\"rank\")==\"2\").toPandas().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dense_rank`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察 `dense_rank` 与 `rank` 的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|dense_rank|\n",
      "+-------+----------+------+----------+\n",
      "|   Joey|     Sales|  9000|         1|\n",
      "|    Ali|     Sales|  8000|         2|\n",
      "|  Elena|     Sales|  8000|         2|\n",
      "|  Cindy|     Sales|  7500|         3|\n",
      "|    Bob|     Sales|  7000|         4|\n",
      "|  Fancy|   Finance| 12000|         1|\n",
      "| George|   Finance| 11000|         2|\n",
      "|   Davd|   Finance| 10000|         3|\n",
      "|  Ilaja| Marketing|  8000|         1|\n",
      "|Haffman| Marketing|  7000|         2|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 注意 rank 排序，8000虽然为同列第二，但7500属于第4名\n",
    "# dense_rank()中， 8000同列第二后，7500属于第3名\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"dense_rank\", F.dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### percent_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些业务场景下，我们需要计算不同数值的百分比排序数据，先来看一个例子吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T03:57:30.642975Z",
     "start_time": "2021-06-30T03:57:17.694910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------+\n",
      "|   name|department|salary|percent_rank|\n",
      "+-------+----------+------+------------+\n",
      "|   Joey|     Sales|  9000|         0.0|\n",
      "|    Ali|     Sales|  8000|        0.25|\n",
      "|  Elena|     Sales|  8000|        0.25|\n",
      "|  Cindy|     Sales|  7500|        0.75|\n",
      "|    Bob|     Sales|  7000|         1.0|\n",
      "|  Fancy|   Finance| 12000|         0.0|\n",
      "| George|   Finance| 11000|         0.5|\n",
      "|   Davd|   Finance| 10000|         1.0|\n",
      "|  Ilaja| Marketing|  8000|         0.0|\n",
      "|Haffman| Marketing|  7000|         1.0|\n",
      "+-------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"percent_rank\",F.percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果可以理解为将 `dense_rank()` 的结果进行归一化，\n",
    "即可得到`0-1`以内的百分数。`percent_rank()` 与 `SQL` 中的 `PERCENT_RANK` 函数效果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ntile()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ntile()`可将分组的数据按照指定数值`n`切分为`n`个部分，\n",
    "每一部分按照行的先后给定相同的序数。例如n指定为2，则将组内数据分为两个部分，\n",
    "第一部分序号为1，第二部分序号为2。理论上两部分数据行数是均等的，\n",
    "但当数据为奇数行时，中间的那一行归到前一部分。\n",
    "\n",
    "按照部门对数据进行分组，然后在组内按照薪水高低进行排序，\n",
    "再使用 `ntile()` 将组内数据切分为两个部分。结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|ntile|\n",
      "+-------+----------+------+-----+\n",
      "|   Joey|     Sales|  9000|    1|\n",
      "|    Ali|     Sales|  8000|    1|\n",
      "|  Elena|     Sales|  8000|    1|\n",
      "|  Cindy|     Sales|  7500|    2|\n",
      "|    Bob|     Sales|  7000|    2|\n",
      "|  Fancy|   Finance| 12000|    1|\n",
      "| George|   Finance| 11000|    1|\n",
      "|   Davd|   Finance| 10000|    2|\n",
      "|  Ilaja| Marketing|  8000|    1|\n",
      "|Haffman| Marketing|  7000|    2|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照部门对数据进行分组，然后在组内按照薪水高低进行排序 \n",
    "windowSpec = Window.partitionBy(\n",
    "    \"department\").orderBy(F.desc(\"salary\"))\n",
    "# 使用ntile() 将组内数据切分为两个部分\n",
    "df.withColumn(\"ntile\", F.ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cume_dist()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cume_dist()`函数用来获取数值的累进分布值，看如下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:00:50.392958Z",
     "start_time": "2021-06-30T04:00:36.883937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------------+\n",
      "|   name|department|salary|         cume_dist|\n",
      "+-------+----------+------+------------------+\n",
      "|   Joey|     Sales|  9000|               0.2|\n",
      "|    Ali|     Sales|  8000|               0.6|\n",
      "|  Elena|     Sales|  8000|               0.6|\n",
      "|  Cindy|     Sales|  7500|               0.8|\n",
      "|    Bob|     Sales|  7000|               1.0|\n",
      "|  Fancy|   Finance| 12000|0.3333333333333333|\n",
      "| George|   Finance| 11000|0.6666666666666666|\n",
      "|   Davd|   Finance| 10000|               1.0|\n",
      "|  Ilaja| Marketing|  8000|               0.5|\n",
      "|Haffman| Marketing|  7000|               1.0|\n",
      "+-------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\n",
    "    \"cume_dist\", F.cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------+\n",
      "|   name|department|salary|percent_rank|\n",
      "+-------+----------+------+------------+\n",
      "|   Joey|     Sales|  9000|         0.0|\n",
      "|    Ali|     Sales|  8000|        0.25|\n",
      "|  Elena|     Sales|  8000|        0.25|\n",
      "|  Cindy|     Sales|  7500|        0.75|\n",
      "|    Bob|     Sales|  7000|         1.0|\n",
      "|  Fancy|   Finance| 12000|         0.0|\n",
      "| George|   Finance| 11000|         0.5|\n",
      "|   Davd|   Finance| 10000|         1.0|\n",
      "|  Ilaja| Marketing|  8000|         0.0|\n",
      "|Haffman| Marketing|  7000|         1.0|\n",
      "+-------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和 percent_rank 对比一下\n",
    "df.withColumn(\n",
    "    'percent_rank',\n",
    "    F.percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果好像和前面的`percent_rank()`很类似对不对，于是我们联想到这个其实也是一种归一化结果，\n",
    "其按照 `rank()` 的结果进行归一化处理。回想一下前面讲过的 `rank()` 函数，并列排序会影响后续排序，\n",
    "于是序号中间可能存在隔断。这样Sales组的排序数就是1、2、2、4、5，\n",
    "归一化以后就得到了0.2、0.6、0.6、0.8、1。这个统计结果按照实际业务来理解就是：\n",
    "- 9000及以上的人占了20%，\n",
    "- 8000及以上的人占了60%，\n",
    "- 7500以上的人数占了80%，\n",
    "- 7000以上的人数占了100%，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lag()` 函数用于寻找按照指定列排好序的分组内每个数值的上一个数值，\n",
    "通俗的说，就是数值排好序以后，寻找排在每个数值的上一个数值。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:05:10.930776Z",
     "start_time": "2021-06-30T04:04:57.823422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|  lag|\n",
      "+-------+----------+------+-----+\n",
      "|   Joey|     Sales|  9000| null|\n",
      "|    Ali|     Sales|  8000| 9000|\n",
      "|  Elena|     Sales|  8000| 8000|\n",
      "|  Cindy|     Sales|  7500| 8000|\n",
      "|    Bob|     Sales|  7000| 7500|\n",
      "|  Fancy|   Finance| 12000| null|\n",
      "| George|   Finance| 11000|12000|\n",
      "|   Davd|   Finance| 10000|11000|\n",
      "|  Ilaja| Marketing|  8000| null|\n",
      "|Haffman| Marketing|  7000| 8000|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 相当于滞后项\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"lag\", F.lag(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lead()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lead()` 用于获取排序后的数值的下一个，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:05:42.528696Z",
     "start_time": "2021-06-30T04:05:29.472542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary| lead|\n",
      "+-------+----------+------+-----+\n",
      "|   Joey|     Sales|  9000| 8000|\n",
      "|    Ali|     Sales|  8000| 8000|\n",
      "|  Elena|     Sales|  8000| 7500|\n",
      "|  Cindy|     Sales|  7500| 7000|\n",
      "|    Bob|     Sales|  7000| null|\n",
      "|  Fancy|   Finance| 12000|11000|\n",
      "| George|   Finance| 11000|10000|\n",
      "|   Davd|   Finance| 10000| null|\n",
      "|  Ilaja| Marketing|  8000| 7000|\n",
      "|Haffman| Marketing|  7000| null|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和滞后项相反，提前一位\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"lead\",F.lead(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 实际业务场景中，假设我们获取了每个月的销售数据，\n",
    "我们可能想要知道，某月份与上一个月或下一个月数据相比怎么样，\n",
    "于是就可以使用`lag`和`lead`来进行数据分析了。\n",
    "1. 思考差分如何做？增长率如何做（同比、环比）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `last()`函数与`first()`函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `last()`返回一组数据中的最后一个元素。\n",
    "- `first()`返回一组数据中的第一个元素。\n",
    "\n",
    "```python\n",
    "F.first(col, ignorenulls=False)\n",
    "F.last(col, ignorenulls=False)\n",
    "```\n",
    "\n",
    "可用于填充，如果当前有值不作处理，若为空值就向前（后）找离他最近的值填充，\n",
    "可以返回上一个值，参数`ignorenulls`默认为`False`，如果设置为`True`，表示只对`null`值应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+\n",
      "|rank| id| age|\n",
      "+----+---+----+\n",
      "|   1|  1|  10|\n",
      "|   2|  1|  20|\n",
      "|   3|  1|  12|\n",
      "|   4|  2|  12|\n",
      "|   4|  2|   1|\n",
      "|   5|  2|null|\n",
      "|   6|  2|  13|\n",
      "|   7|  3|   2|\n",
      "|   8|  3|null|\n",
      "+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1,1,'10'), (2,1,'20'), (3,1,'12'), (4,2,'12'),\n",
    "    (4, 2, '1'), (5,2,None), (6, 2, '13'), (7,3,'2'), (8,3,None)],\n",
    "    ['rank','id','age'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|rank| id|age|\n",
      "+----+---+---+\n",
      "|   1|  1| 10|\n",
      "|   2|  1| 10|\n",
      "|   3|  1| 10|\n",
      "|   7|  3|  2|\n",
      "|   8|  3|  2|\n",
      "|   4|  2| 12|\n",
      "|   4|  2| 12|\n",
      "|   5|  2| 12|\n",
      "|   6|  2| 12|\n",
      "+----+---+---+\n",
      "\n",
      "+----+---+---+\n",
      "|rank| id|age|\n",
      "+----+---+---+\n",
      "|   1|  1| 10|\n",
      "|   2|  1| 20|\n",
      "|   3|  1| 12|\n",
      "|   7|  3|  2|\n",
      "|   8|  3|  2|\n",
      "|   4|  2|  1|\n",
      "|   4|  2|  1|\n",
      "|   5|  2|  1|\n",
      "|   6|  2| 13|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以该组第一个作为填充\n",
    "df.withColumn('age', F.first('age', True).over(\n",
    "    Window.partitionBy('id').orderBy('rank'))).show()\n",
    "\n",
    "# 以该组最后一个作为填充\n",
    "df.withColumn('age', F.last('age', True).over(\n",
    "    Window.partitionBy('id').orderBy('rank'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的聚合函数有`avg, sum, min, max, count, approx_count_distinct()`等，我们用如下代码来同时使用这些函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:09:45.472304Z",
     "start_time": "2021-06-30T04:09:31.098191Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`department`' given input columns: [age, id, rank];;\n'Project [rank#223L, id#224L, age#225, row_number() windowspecdefinition('department, 'salary DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row#1079]\n+- LogicalRDD [rank#223L, id#224L, age#225], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-43569a2f34fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwindowSpecAgg\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"department\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"row\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindowSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"avg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindowSpecAgg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindowSpecAgg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chensy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2094\u001b[0m         \"\"\"\n\u001b[0;32m   2095\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chensy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chensy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chensy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`department`' given input columns: [age, id, rank];;\n'Project [rank#223L, id#224L, age#225, row_number() windowspecdefinition('department, 'salary DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row#1079]\n+- LogicalRDD [rank#223L, id#224L, age#225], false\n"
     ]
    }
   ],
   "source": [
    "# 分组，并对组内数据排序\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "# 仅分组\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", F.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", F.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", F.min(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", F.max(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"count\", F.count(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"distinct_count\", F.approxCountDistinct(\"salary\").over(windowSpecAgg)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是 `approx_count_distinct()` 函数适用与窗函数的统计，\n",
    "而在`groupby`中通常用`countDistinct()`来代替该函数，用来求组内不重复的数值的条数。\n",
    "\n",
    "从结果来看，统计值基本上是按照部门分组，统计组内的salary情况。\n",
    "\n",
    "如果我们只想要保留部门的统计结果，而将每个人的实际情况去掉，可以采用如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T04:12:31.891141Z",
     "start_time": "2021-06-30T04:12:17.217795Z"
    }
   },
   "outputs": [],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df = df.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", F.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", F.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", F.min(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", F.max(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"count\", F.count(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"distinct_count\", F.approx_count_distinct(\"salary\").over(windowSpecAgg))\n",
    "\n",
    "# 仅选取分组第一行数据\n",
    "# 用F.col 去选row 行，怪怪的\n",
    "df_part  = df.where(F.col(\"row\")==1)\n",
    "df_part.select(\"department\",\"avg\",\"sum\",\"min\",\"max\",\"count\",\"distinct_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T02:25:11.698388Z",
     "start_time": "2021-06-30T02:25:11.658386Z"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
