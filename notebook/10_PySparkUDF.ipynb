{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://pic4.zhimg.com/80/v2-5db1a82996ec388725185ae900a58008.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是UDF？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF 用户定义函数，`PySpark UDF` 类似于传统数据库上的 `UDF`。 `PySpark SQL Functions` 不能满足业务要求时，需要使用 UDF 进行自定义函数。\n",
    "\n",
    "一般步骤是，首先使用 `Python` 语法创建一个函数，并使用 `PySpark SQL` 包装它为`udf()`，然后在 `DataFrame` 上使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么需要UDF？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF 用于扩展框架的功能并在多个 DataFrame 上重用这些功能。\n",
    "\n",
    "例如，您想将名称字符串中单词的每个首字母都转换为大写；\n",
    "`PySpark` 没有此函数，您可以创建 `UDF`，并根据需要在多个`DataFrame`上重用它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 `PySpark UDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先创建一个 `PySpark DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 Python 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 `Python` 函数。它接受一个字符串参数并将每个单词的第一个字母转换为大写字母。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Jones'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertCase(string):\n",
    "    resStrArr=[]\n",
    "    stringArr = string.split(\" \")\n",
    "    for x in stringArr:\n",
    "        resStrArr.append(f\"{x[0].upper()}{x[1:]}\")\n",
    "    return ' '.join(resStrArr)\n",
    "\n",
    "# 测试一下\n",
    "convertCase('john jones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 `Python` 函数转换为 `PySpark UDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在`convertCase()`通过将函数传递给 `PySpark SQL` 来将此函数转换为 `UDF`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方式 一：lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returnType 为返回数据的数据类型\n",
    "convert_udf_lambda = F.udf(lambda z: convertCase(z), returnType=StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方式二：直接传入函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_udf = F.udf(f=convertCase, returnType=StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方式三：装饰器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def convertCaseDecorate(string):\n",
    "    resStrArr=[]\n",
    "    stringArr = string.split(\" \")\n",
    "    for x in stringArr:\n",
    "        resStrArr.append(f\"{x[0].upper()}{x[1:]}\")\n",
    "    return ' '.join(resStrArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在 `DataFrame` 中使用 `UDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 `PySpark DataFrame select()` 中使用 `UDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  John Jones|\n",
      "|    2|Tracey Smith|\n",
      "|    3| Amy Sanders|\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  John Jones|\n",
      "|    2|Tracey Smith|\n",
      "|    3| Amy Sanders|\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  John Jones|\n",
      "|    2|Tracey Smith|\n",
      "|    3| Amy Sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lambda UDF\n",
    "df.select(F.col(\"Seqno\"), convert_udf_lambda(F.col(\"Name\")).alias(\"Name\")).show()\n",
    "# functions UDF\n",
    "df.select(F.col(\"Seqno\"), convert_udf(F.col(\"Name\")).alias(\"Name\")).show()\n",
    "# 装饰器 UDF\n",
    "df.select(F.col(\"Seqno\"), convertCaseDecorate(F.col(\"Name\")).alias(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上面三种`UDF`的结果都是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 `PySpark DataFrame withColumn()` 中使用 `UDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |John Jones   |\n",
      "|2    |tracey smith|Tracey Smith |\n",
      "|3    |amy sanders |Amy Sanders  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Name\", convert_udf(F.col(\"Name\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注册 `PySpark UDF` 并在 `SQL` 上使用4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了`convertCase()`在 `PySpark SQL` 上使用函数，您需要使用`spark.udf.register()`。\n",
    "\n",
    "```python\n",
    "spark.udf.register(\"convert_udf\", convertCase, StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\").show(truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 空值检查\n",
    "\n",
    "当您有一列包含`null`记录的值时，如果设计不仔细，`UDF` 很容易出错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df2.withColumn(\"Cureated Name\", convert_udf(F.col(\"Name\"))).show(truncate=False)\n",
    "\n",
    "AttributeError: 'NoneType' object has no attribute 'split'\n",
    "```\n",
    "\n",
    "请注意，从上面的代码片段中，`Seqno 4`的`name`值为`None`。\n",
    "由于 `UDF` 函数没有处理 `null`，因此在 `DataFrame` 上使用它会返回错误。\n",
    "在 `Python` 中 `None` 被认为是 `null`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下修改`UDF`以应对空值情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|        Name|Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|    1|  john jones|   John Jones|\n",
      "|    2|tracey smith| Tracey Smith|\n",
      "|    3| amy sanders|  Amy Sanders|\n",
      "|    4|        null|         null|\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def convertCaseDecorate(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    else:\n",
    "        resStrArr=[]\n",
    "        stringArr = string.split(\" \")\n",
    "        for x in stringArr:\n",
    "            resStrArr.append(f\"{x[0].upper()}{x[1:]}\")\n",
    "        return ' '.join(resStrArr)\n",
    "\n",
    "df2.withColumn(\"Cureated Name\", convertCaseDecorate(F.col(\"Name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T05:42:12.429411Z",
     "start_time": "2021-06-30T05:42:12.362964Z"
    }
   },
   "source": [
    "## `UDF` 输入输出结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One in One out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以一列作为输入，输出为另一列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+-------------+\n",
      "|Seqno|        Name|Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|    1|  john jones|   John Jones|\n",
      "|    2|tracey smith| Tracey Smith|\n",
      "|    3| amy sanders|  Amy Sanders|\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def convertCaseDecorate(string):\n",
    "    resStrArr=[]\n",
    "    stringArr = string.split(\" \")\n",
    "    for x in stringArr:\n",
    "        resStrArr.append(f\"{x[0].upper()}{x[1:]}\")\n",
    "    return ' '.join(resStrArr)\n",
    "\n",
    "df = df.withColumn(\"Cureated Name\", convertCaseDecorate(F.col(\"Name\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many In One Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以两列或多个列为输入，以另一列作为输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------------+\n",
      "|Seqno|        Name|       Cureated Name|\n",
      "+-----+------------+--------------------+\n",
      "|    1|  john jones|john jones-John J...|\n",
      "|    2|tracey smith|tracey smith-Trac...|\n",
      "|    3| amy sanders|amy sanders-Amy S...|\n",
      "+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def ManyInOneOut(name_1, name_2):\n",
    "    return f'{name_1}-{name_2}'\n",
    "\n",
    "df.withColumn(\n",
    "    \"Cureated Name\", \n",
    "    ManyInOneOut(F.col(\"Name\"), F.col(\"Cureated Name\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many In Many Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以两列或多个列为输入，以多个列作为输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:37:04.518487Z",
     "start_time": "2021-06-30T11:36:51.345394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+\n",
      "|  a|  b|  calculate|\n",
      "+---+---+-----------+\n",
      "|1.0|2.0|[3.0, -1.0]|\n",
      "|2.0|4.0|[6.0, -2.0]|\n",
      "+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"sum\", FloatType(), False),\n",
    "    StructField(\"diff\", FloatType(), False)])\n",
    "\n",
    "@F.udf(returnType=schema)\n",
    "def sum_diff(f1, f2):\n",
    "    return [f1 + f2, f1-f2]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame([[1., 2.], [2., 4.]], columns=['a', 'b']))\n",
    "\n",
    "df_new = df.withColumn(\"calculate\", sum_diff(F.col('a'), F.col('b')))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: double (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- calculate: struct (nullable = true)\n",
      " |    |-- sum: float (nullable = false)\n",
      " |    |-- diff: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 最终表结构\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:50:40.766009Z",
     "start_time": "2021-06-30T11:50:27.790803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+---+----+---+----+\n",
      "|  a|  b|  calculate|sum|diff|sum|diff|\n",
      "+---+---+-----------+---+----+---+----+\n",
      "|1.0|2.0|[3.0, -1.0]|3.0|-1.0|3.0|-1.0|\n",
      "|2.0|4.0|[6.0, -2.0]|6.0|-2.0|6.0|-2.0|\n",
      "+---+---+-----------+---+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.select('*', 'calculate.*', 'calculate.sum', 'calculate.diff').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 闭包构造`UDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T08:02:52.185857Z",
     "start_time": "2021-03-15T08:02:51.957242Z"
    }
   },
   "source": [
    "当我们想传入`UDF`两个参数时，其中一个参数为固定参数，就像下面的示例，需要向`state_abbreviation`函数传入`s`与`mapping`参数，以期望用字典`mapping`中的键值对信息替换`s`中的信息，使用以下构造方式进行运算。\n",
    "```python\n",
    "@F.udf(returnType=StringType())\n",
    "def state_abbreviation(s, mapping):\n",
    "    if s is not None:\n",
    "        return mapping[s]\n",
    "\n",
    "df = spark.createDataFrame([['Alabama',], ['Texas',], ['Antioquia',]]).toDF('state')\n",
    "mapping = {'Alabama': 'AL', 'Texas': 'TX'}\n",
    "df.withColumn('state_abbreviation', state_abbreviation(F.col('state'), mapping)).show()\n",
    "```\n",
    "\n",
    "> 会报出以下错误\n",
    "```python\n",
    "TypeError: Invalid argument, not a string or column: {'Alabama': 'AL', 'Texas': 'TX'} of type <class 'dict'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考虑使用闭包方式构造`UDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T08:03:14.395087Z",
     "start_time": "2021-03-15T08:03:02.349846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|    state|state_abbreviation|\n",
      "+---------+------------------+\n",
      "|  Alabama|                AL|\n",
      "|    Texas|                TX|\n",
      "|Antioquia|              null|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['Alabama',], ['Texas',], ['Antioquia',]]).toDF('state')\n",
    "\n",
    "def working_fun(mapping):\n",
    "    def f(x):\n",
    "        return mapping.get(x)\n",
    "    return F.udf(f)\n",
    "\n",
    "mapping = {'Alabama': 'AL', 'Texas': 'TX'}\n",
    "df.withColumn('state_abbreviation', working_fun(mapping)(F.col('state'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考虑使用`broadcast`方式进行运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T09:51:03.589967Z",
     "start_time": "2021-03-02T09:50:52.916942Z"
    },
    "scrolled": true
   },
   "source": [
    "该方法通过 `spark.sparkContext.broadcast` 将 `mapping` 广播到全部运算节点上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|    state|state_abbreviation|\n",
      "+---------+------------------+\n",
      "|  Alabama|                AL|\n",
      "|    Texas|                TX|\n",
      "|Antioquia|              null|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def working_fun(x):\n",
    "    return mapping_broadcasted.value.get(x)\n",
    "\n",
    "mapping_broadcasted = spark.sparkContext.broadcast(mapping)\n",
    "df.withColumn('state_abbreviation', working_fun(F.col('state'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在`GroupBy`中使用`UDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下计算每个人的贷款总合，以`UDF`的形式进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name|loan|\n",
      "+---+----+----+\n",
      "|  1| bob|  10|\n",
      "|  1| bob|  20|\n",
      "|  1| bob|  19|\n",
      "|  1| bob|  20|\n",
      "|  2| nic|  11|\n",
      "|  1| nic|   8|\n",
      "|  2| nic|  11|\n",
      "|  1| nic|   9|\n",
      "|  3| ace|  12|\n",
      "|  1| ace|  20|\n",
      "|  3| ace|   1|\n",
      "|  1| ace|  20|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [['1', 'bob', 10], ['1', 'bob', 20],\n",
    "     ['1', 'bob', 19], ['1', 'bob', 20],\n",
    "     ['2', 'nic', 11], ['1', 'nic', 8],\n",
    "     ['2', 'nic', 11], ['1', 'nic', 9],\n",
    "     ['3', 'ace', 12], ['1', 'ace', 20],\n",
    "     ['3', 'ace', 1], ['1', 'ace', 20],],\n",
    "    ['id', 'name', 'loan'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|name|            loan|\n",
      "+----+----------------+\n",
      "| nic|  [11, 8, 11, 9]|\n",
      "| ace| [12, 20, 1, 20]|\n",
      "| bob|[10, 20, 19, 20]|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 首先进行分组，汇总每个人的贷款金额\n",
    "df_group = df.groupBy('name').agg(F.collect_list('loan').alias('loan'))\n",
    "df_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---+\n",
      "|name|            loan|sum|\n",
      "+----+----------------+---+\n",
      "| nic|  [11, 8, 11, 9]| 39|\n",
      "| ace| [12, 20, 1, 20]| 53|\n",
      "| bob|[10, 20, 19, 20]| 69|\n",
      "+----+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义 UDF \n",
    "@F.udf(returnType=IntegerType())\n",
    "def func(array):\n",
    "    return sum(array)\n",
    "\n",
    "df_group = df_group.withColumn('sum', func(F.col('loan')))\n",
    "df_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "zh-cn",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh-cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
