{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3998ae14",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5a900",
   "metadata": {},
   "source": [
    "在 Spark 的早期版本中，SparkContext 是 Spark 的主要切入点，由于 RDD 是主要的 API，我 们通过 sparkContext来创建和操作 RDD。对于每个其他的 API，我们需要使用不同的 context。 例如：\n",
    "\n",
    "- 对于 Spark Streaming，我们需要使用 StreamingContext\n",
    "\n",
    "- 对于 Spark SQL，使用 SQLContext\n",
    "\n",
    "- 对于 Hive，使用 HiveContext "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc353947",
   "metadata": {},
   "source": [
    "- pyspark.SparkContext: Spark 库的主要入口点，它表示与Spark集群的一个连接，其他重要的对象都要依赖它.SparkContext存在于Driver中，是Spark功能的主要入口。\n",
    "代表着与Spark集群的连接，可以在集群上创建RDD，accumulators和广播变量\n",
    "- pyspark.RDD: 是Spark的主要数据抽象概念，是Spark库中定义的一个抽象类。\n",
    "- pyspark.streaming.StreamingContext 一个定义在Spark Streaming库中定义的类, 每一个Spark Streaming 应用都必须创建这个类\n",
    "- pyspark.streaming.DStrem：离散数据流，是Spark Streaming处理数据流的主要对象\n",
    "- pyspark.sql.SparkSession: 是DataFrame和SQL函数的主要入口点。\n",
    "- pyspark.sql.DataFrame: 是Spark SQL的主要抽象对象，若干行的分布式数据，每一行都要若干个有名字的列。 跟R/Python中的DataFrame 相像\n",
    ",有着更丰富的优化。DataFrame可以有很多种方式进行构造，例如： 结构化数据文件，Hive的table, 外部数据库，RDD。\n",
    "- pyspark.sql.Column DataFrame 的列表达.\n",
    "- pyspark.sql.Row DataFrame的行数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b482db",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate() \n",
    "    ## 获取或者新建一个 sparkSession\n",
    "    #spark master URL. 本地为local, “local[4]” 本地4核,\n",
    "    # or “spark://master:7077” to run on a Spark standalone cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3383d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate() \n",
    "    ## 获取或者新建一个 sparkSession\n",
    "    #spark master URL. 本地为local, “local[4]” 本地4核,\n",
    "    # or “spark://master:7077” to run on a Spark standalone cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407ab17",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
