{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb70561b",
   "metadata": {},
   "source": [
    "# PySpark 分解数组到列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cb851",
   "metadata": {},
   "source": [
    "使用不同的 `PySpark DataFrame` 函数分解数组或列表并映射到列。\n",
    "- `explode, explode_outer, poseexplode, posexplode_outer`\n",
    "\n",
    "在开始之前，让我们创建一个带有数组和字典字段的 `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a0d3e",
   "metadata": {},
   "source": [
    "## 创建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd7f986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+--------------+--------------------+\n",
      "|      name|knownLanguages|          properties|\n",
      "+----------+--------------+--------------------+\n",
      "|     James| [Java, Scala]|[eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java,]|[eye ->, hair -> ...|\n",
      "|    Robert|    [CSharp, ]|[eye -> , hair ->...|\n",
      "|Washington|          null|                null|\n",
      "| Jefferson|        [1, 2]|                  []|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data=arrayData, \n",
    "    schema=['name','knownLanguages','properties'])\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb049b40",
   "metadata": {},
   "source": [
    "## `explode`将数组列映射到列\n",
    "\n",
    "`PySpark` 函数 `explode(e: Column)` 用于分解数组到列。\n",
    "当一个数组传递给这个函数时，它会创建一个新的默认列`col1`，\n",
    "它包含所有数组元素。当一个映射被传递时，它会创建两个新列，\n",
    "一个是键，一个是值，映射中的每个元素都分成行。\n",
    "\n",
    "`explode`将忽略具有 `null` 或空的元素。从上面的例子中，\n",
    "`Washington` 和 `Jefferson` 在数组和映射中有空值，\n",
    "因此下面的代码片段不包含这些行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69707d1c",
   "metadata": {},
   "source": [
    "### 列表转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df995a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d22559",
   "metadata": {},
   "source": [
    "### 字典转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42bf7ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------+----+-----+\n",
      "|   name| key|value|\n",
      "+-------+----+-----+\n",
      "|  James| eye|brown|\n",
      "|  James|hair|black|\n",
      "|Michael| eye| null|\n",
      "|Michael|hair|brown|\n",
      "| Robert| eye|     |\n",
      "| Robert|hair|  red|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df3 = df.select(df.name, explode(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118d544",
   "metadata": {},
   "source": [
    "### `explode_outer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f976e03",
   "metadata": {},
   "source": [
    "`explode_outer(e: Column)` 函数用于为数组或映射列中的每个元素创建一行。\n",
    "与`explode`不同，如果数组或`map`为空，`explode_outer`返回`null`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae04757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      name|   col|\n",
      "+----------+------+\n",
      "|     James|  Java|\n",
      "|     James| Scala|\n",
      "|   Michael| Spark|\n",
      "|   Michael|  Java|\n",
      "|   Michael|  null|\n",
      "|    Robert|CSharp|\n",
      "|    Robert|      |\n",
      "|Washington|  null|\n",
      "| Jefferson|     1|\n",
      "| Jefferson|     2|\n",
      "+----------+------+\n",
      "\n",
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|     |\n",
      "|    Robert|hair|  red|\n",
      "|Washington|null| null|\n",
      "| Jefferson|null| null|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
    "\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,explode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de74a2",
   "metadata": {},
   "source": [
    "## `poseexplode`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943cf2bc",
   "metadata": {},
   "source": [
    "`posexplode(e: Column)` 为数组中的每个元素创建一行，\n",
    "并创建列`pos`来保存数组元素的位置和列`col`来保存实际的数组值。\n",
    "当输入列是`map`时，`posexplode` 函数创建列`pos`来保存`map`元素的位置，`key`和`value`列为键值。\n",
    "\n",
    "注意该函数忽略具有 `null` 的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7d4d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     name|pos|   col|\n",
      "+---------+---+------+\n",
      "|    James|  0|  Java|\n",
      "|    James|  1| Scala|\n",
      "|  Michael|  0| Spark|\n",
      "|  Michael|  1|  Java|\n",
      "|  Michael|  2|  null|\n",
      "|   Robert|  0|CSharp|\n",
      "|   Robert|  1|      |\n",
      "|Jefferson|  0|     1|\n",
      "|Jefferson|  1|     2|\n",
      "+---------+---+------+\n",
      "\n",
      "+-------+---+----+-----+\n",
      "|   name|pos| key|value|\n",
      "+-------+---+----+-----+\n",
      "|  James|  0| eye|brown|\n",
      "|  James|  1|hair|black|\n",
      "|Michael|  0| eye| null|\n",
      "|Michael|  1|hair|brown|\n",
      "| Robert|  0| eye|     |\n",
      "| Robert|  1|hair|  red|\n",
      "+-------+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,posexplode(df.knownLanguages)).show()\n",
    "\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,posexplode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f1a2d",
   "metadata": {},
   "source": [
    "## posexplode_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067fbaf",
   "metadata": {},
   "source": [
    "`posexplode_outer(e: Column)` 返回带有空值的行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531a4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|      name| pos|   col|\n",
      "+----------+----+------+\n",
      "|     James|   0|  Java|\n",
      "|     James|   1| Scala|\n",
      "|   Michael|   0| Spark|\n",
      "|   Michael|   1|  Java|\n",
      "|   Michael|   2|  null|\n",
      "|    Robert|   0|CSharp|\n",
      "|    Robert|   1|      |\n",
      "|Washington|null|  null|\n",
      "| Jefferson|   0|     1|\n",
      "| Jefferson|   1|     2|\n",
      "+----------+----+------+\n",
      "\n",
      "+----------+----+----+-----+\n",
      "|      name| pos| key|value|\n",
      "+----------+----+----+-----+\n",
      "|     James|   0| eye|brown|\n",
      "|     James|   1|hair|black|\n",
      "|   Michael|   0| eye| null|\n",
      "|   Michael|   1|hair|brown|\n",
      "|    Robert|   0| eye|     |\n",
      "|    Robert|   1|hair|  red|\n",
      "|Washington|null|null| null|\n",
      "| Jefferson|null|null| null|\n",
      "+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(\"name\",posexplode_outer(\"knownLanguages\")).show()\n",
    "\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,posexplode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a9657",
   "metadata": {},
   "source": [
    "## 将嵌套数组 `DataFrame` 列分解为行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fb594",
   "metadata": {},
   "source": [
    "创建一个带有嵌套数组列的 `DataFrame`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5d0795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subjects: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-------+-----------------------------------+\n",
      "|name   |subjects                           |\n",
      "+-------+-----------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
      "+-------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcc7d4",
   "metadata": {},
   "source": [
    "### 展平数组，请使用 `flatten` 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c51caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|name   |flatten(subjects)              |\n",
      "+-------+-------------------------------+\n",
      "|James  |[Java, Scala, C++, Spark, Java]|\n",
      "|Michael|[Spark, Java, C++, Spark, Java]|\n",
      "|Robert |[CSharp, VB, Spark, Python]    |\n",
      "+-------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "df.select(df.name, flatten(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a063e25",
   "metadata": {},
   "source": [
    "### 展平再分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c878598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|name   |col   |\n",
      "+-------+------+\n",
      "|James  |Java  |\n",
      "|James  |Scala |\n",
      "|James  |C++   |\n",
      "|James  |Spark |\n",
      "|James  |Java  |\n",
      "|Michael|Spark |\n",
      "|Michael|Java  |\n",
      "|Michael|C++   |\n",
      "|Michael|Spark |\n",
      "|Michael|Java  |\n",
      "|Robert |CSharp|\n",
      "|Robert |VB    |\n",
      "|Robert |Spark |\n",
      "|Robert |Python|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name, explode(flatten(df.subjects))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647a900",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
