{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = spark.createDataFrame(\n",
    "    [(1,), (1,), (2,), (2,), (3, ), (4, ), (4, )], ['id'])\n",
    "\n",
    "df_b = spark.createDataFrame(\n",
    "    [(1,), (1, ), (3, )], ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  2|\n",
      "|  4|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.join(df_b, on='id', how='left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  2|\n",
      "|  4|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.join(df_b, on='id', how='leftanti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union 上下并表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unionByName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `unionByName` 根据列名进行行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T10:44:07.992405Z",
     "start_time": "2021-06-30T10:43:45.436935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   6|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[1,2,3]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[4,5,6]], [\"col1\", \"col2\", \"col0\"])\n",
    "df_unionbyname = df1.unionByName(df2)\n",
    "df_unionbyname.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## unionAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1.unionAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join 左右连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建基本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T10:46:43.833513Z",
     "start_time": "2021-06-30T10:46:30.068866Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----------+-------------+\n",
      "|tId|custId|date|      city|           _5|\n",
      "+---+------+----+----------+-------------+\n",
      "|  1|     5|   3|01/01/2015|San Francisco|\n",
      "|  2|     6|   1|01/02/2015|     San Jose|\n",
      "|  3|     1|   6|01/01/2015|       Boston|\n",
      "|  4|   200| 400|01/02/2015|    Palo Alto|\n",
      "|  6|   100| 100|01/02/2015|Mountain View|\n",
      "+---+------+----+----------+-------------+\n",
      "\n",
      "+---+--------+---+------+\n",
      "|cID|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 21|     M|\n",
      "|  2|     Liz| 25|     F|\n",
      "|  3|    John| 31|     M|\n",
      "|  4|Jennifer| 45|     F|\n",
      "|  5|  Robert| 41|     M|\n",
      "|  6|  Sandra| 45|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    (1,5,3,\"01/01/2015\", \"San Francisco\"), \n",
    "    (2,6,1, \"01/02/2015\", \"San Jose\"),\n",
    "    (3,1,6,\"01/01/2015\", \"Boston\"), \n",
    "    (4,200,400,\"01/02/2015\",\"Palo Alto\"),     \n",
    "    (6, 100, 100, \"01/02/2015\", \"Mountain View\")]\n",
    "df_transactions = spark.createDataFrame(\n",
    "    transactions, ['tId', \"custId\", \"date\", \"city\"])\n",
    "df_transactions.show()\n",
    "\n",
    "customers =  [\n",
    "    (1,'James',21,'M'), (2, \"Liz\",25,\"F\"), (3, \"John\", 31, \"M\"),\n",
    "    (4, \"Jennifer\", 45, \"F\"), (5, \"Robert\", 41, \"M\"), (6, \"Sandra\", 45, \"F\")]\n",
    "df_customers = spark.createDataFrame(\n",
    "    customers, [\"cID\", \"name\", \"age\", \"gender\"]) # list -> DF\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----------+-------------+----+------+----+------+\n",
      "|tId|custId|date|      city|           _5| cID|  name| age|gender|\n",
      "+---+------+----+----------+-------------+----+------+----+------+\n",
      "|  2|     6|   1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n",
      "|  1|     5|   3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n",
      "|  3|     1|   6|01/01/2015|       Boston|   1| James|  21|     M|\n",
      "|  6|   100| 100|01/02/2015|Mountain View|null|  null|null|  null|\n",
      "|  4|   200| 400|01/02/2015|    Palo Alto|null|  null|null|  null|\n",
      "+---+------+----+----------+-------------+----+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join_left = df_transactions.join(\n",
    "    df_customers, df_transactions.custId == df_customers.cID, \n",
    "    \"left_outer\")\n",
    "df_join_left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----------+-------------+----+------+----+------+\n",
      "|tId|custId|date|      city|           _5| cID|  name| age|gender|\n",
      "+---+------+----+----------+-------------+----+------+----+------+\n",
      "|  2|     6|   1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n",
      "|  1|     5|   3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n",
      "|  3|     1|   6|01/01/2015|       Boston|   1| James|  21|     M|\n",
      "|  6|   100| 100|01/02/2015|Mountain View|null|  null|null|  null|\n",
      "|  4|   200| 400|01/02/2015|    Palo Alto|null|  null|null|  null|\n",
      "+---+------+----+----------+-------------+----+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join_left = df_transactions.join(\n",
    "    df_customers, df_transactions.custId == df_customers.cID, \"left\")\n",
    "df_join_left.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df_join_inner = df_transactions.join(\n",
    "    df_customers, df_transactions.custId == df_customers.cID, \"inner\")\n",
    "df_join_inner.show()\n",
    "+---+------+----+----------+-------------+---+------+---+------+\n",
    "|tId|custId|date|      city|           _5|cID|  name|age|gender|\n",
    "+---+------+----+----------+-------------+---+------+---+------+\n",
    "|  2|     6|   1|01/02/2015|     San Jose|  6|Sandra| 45|     F|\n",
    "|  1|     5|   3|01/01/2015|San Francisco|  5|Robert| 41|     M|\n",
    "|  3|     1|   6|01/01/2015|       Boston|  1| James| 21|     M|\n",
    "+---+------+----+----------+-------------+---+------+---+------+\n",
    "\n",
    "df_join_outer = df_transactions.join(\n",
    "    df_customers, df_transactions.custId == df_customers.cID, \"outer\")\n",
    "df_join_outer.show()\n",
    "| tId|custId|date|      city|           _5| cID|    name| age|gender|\n",
    "+----+------+----+----------+-------------+----+--------+----+------+\n",
    "|   2|     6|   1|01/02/2015|     San Jose|   6|  Sandra|  45|     F|\n",
    "|   1|     5|   3|01/01/2015|San Francisco|   5|  Robert|  41|     M|\n",
    "|   3|     1|   6|01/01/2015|       Boston|   1|   James|  21|     M|\n",
    "|   6|   100| 100|01/02/2015|Mountain View|null|    null|null|  null|\n",
    "|null|  null|null|      null|         null|   3|    John|  31|     M|\n",
    "|   4|   200| 400|01/02/2015|    Palo Alto|null|    null|null|  null|\n",
    "|null|  null|null|      null|         null|   2|     Liz|  25|     F|\n",
    "|null|  null|null|      null|         null|   4|Jennifer|  45|     F|\n",
    "\n",
    "df_join_left = df_transactions.join(\n",
    "    df_customers, df_transactions.custId == df_customers.cID, \"left_outer\")\n",
    "df_join_left.show()\n",
    "+---+------+----+----------+-------------+----+------+----+------+\n",
    "|tId|custId|date|      city|           _5| cID|  name| age|gender|\n",
    "+---+------+----+----------+-------------+----+------+----+------+\n",
    "|  2|     6|   1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n",
    "|  1|     5|   3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n",
    "|  3|     1|   6|01/01/2015|       Boston|   1| James|  21|     M|\n",
    "|  6|   100| 100|01/02/2015|Mountain View|null|  null|null|  null|\n",
    "|  4|   200| 400|01/02/2015|    Palo Alto|null|  null|null|  null|\n",
    "+---+------+----+----------+-------------+----+------+----+------+\n",
    "\n",
    "df_join_right = df_transactions.join(\n",
    "    df_customers, df_transactions.custId == df_customers.cID, \"right_outer\")\n",
    "df_join_right.show()\n",
    "+----+------+----+----------+-------------+---+--------+---+------+\n",
    "| tId|custId|date|      city|           _5|cID|    name|age|gender|\n",
    "+----+------+----+----------+-------------+---+--------+---+------+\n",
    "|   2|     6|   1|01/02/2015|     San Jose|  6|  Sandra| 45|     F|\n",
    "|   1|     5|   3|01/01/2015|San Francisco|  5|  Robert| 41|     M|\n",
    "|   3|     1|   6|01/01/2015|       Boston|  1|   James| 21|     M|\n",
    "|null|  null|null|      null|         null|  3|    John| 31|     M|\n",
    "|null|  null|null|      null|         null|  2|     Liz| 25|     F|\n",
    "|null|  null|null|      null|         null|  4|Jennifer| 45|     F|\n",
    "+----+------+----+----------+-------------+---+--------+---+------+\n",
    "\n",
    "## left_semi 返回在两个表都有的行，只返回左表\n",
    "## left_anti 返回只在左表有的行\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字段合并与分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lag\\lead\n",
    "- explode\\explode_outer 将一行分成多行\n",
    "\n",
    "二者可用来实现一行分为多行：为给定数组或映射中的每个元素返回新行。除非另有指定，否则对数组中的元素使用默认列名col，对映射中的元素使用key和value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+\n",
      "| id|  an_array|   a_map|\n",
      "+---+----------+--------+\n",
      "|  1|[foo, bar]|[x -> 1]|\n",
      "|  2|        []|      []|\n",
      "|  3|      null|    null|\n",
      "+---+----------+--------+\n",
      "\n",
      "+---+----------+----+-----+\n",
      "| id|  an_array| key|value|\n",
      "+---+----------+----+-----+\n",
      "|  1|[foo, bar]|   x|    1|\n",
      "|  2|        []|null| null|\n",
      "|  3|      null|null| null|\n",
      "+---+----------+----+-----+\n",
      "\n",
      "+---+----------+---+-----+\n",
      "| id|  an_array|key|value|\n",
      "+---+----------+---+-----+\n",
      "|  1|[foo, bar]|  x|    1|\n",
      "+---+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "\n",
    "df.show()\n",
    "df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
    "df.select(\"id\", \"an_array\", explode(\"a_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---+\n",
      "|gid|      score|  e|\n",
      "+---+-----------+---+\n",
      "| a1|90 80 79 80| 90|\n",
      "| a1|90 80 79 80| 80|\n",
      "| a1|90 80 79 80| 79|\n",
      "| a1|90 80 79 80| 80|\n",
      "| a2|90 80 79 80| 90|\n",
      "| a2|90 80 79 80| 80|\n",
      "| a2|90 80 79 80| 79|\n",
      "| a2|90 80 79 80| 80|\n",
      "| a3|90 80 79 80| 90|\n",
      "| a3|90 80 79 80| 80|\n",
      "| a3|90 80 79 80| 79|\n",
      "| a3|90 80 79 80| 80|\n",
      "+---+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_explode = df.withColumn(\"e\", explode(split(df['score'], \" \")))\n",
    "df_explode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 列数据的分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ('a1', '90 80 79 80'),\n",
    "        ('a2', '90 80 79 80'),\n",
    "        ('a3', '90 80 79 80'),\n",
    "    ],\n",
    "    (\"gid\", \"score\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------------+\n",
      "|gid|      score|               s|\n",
      "+---+-----------+----------------+\n",
      "| a1|90 80 79 80|[90, 80, 79, 80]|\n",
      "| a2|90 80 79 80|[90, 80, 79, 80]|\n",
      "| a3|90 80 79 80|[90, 80, 79, 80]|\n",
      "+---+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split = df.withColumn(\"s\", split(df['score'], \" \"))\n",
    "df_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### zipWithIndex:给每个元素生成一个索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['a', 'b'], 2).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新增列的个数 4\n",
      "列名： [('score_0', 0), ('score_1', 1), ('score_2', 2), ('score_3', 3)]\n",
      "+---+-----------+----------------+-------+-------+-------+-------+\n",
      "|gid|      score|               s|score_0|score_1|score_2|score_3|\n",
      "+---+-----------+----------------+-------+-------+-------+-------+\n",
      "| a1|90 80 79 80|[90, 80, 79, 80]|     90|     80|     79|     80|\n",
      "| a2|90 80 79 80|[90, 80, 79, 80]|     90|     80|     79|     80|\n",
      "| a3|90 80 79 80|[90, 80, 79, 80]|     90|     80|     79|     80|\n",
      "+---+-----------+----------------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.first()\n",
    "numAttrs = len(first_row['score'].split(\" \"))\n",
    "print(\"新增列的个数\", numAttrs)\n",
    "\n",
    "attrs = sc.parallelize(\n",
    "    [\"score_\" + str(i) for i in range(numAttrs)]\n",
    ").zipWithIndex().collect()\n",
    "print(\"列名：\", attrs)\n",
    "\n",
    "for name, index in attrs:\n",
    "    df_split = df_split.withColumn(name, df_split['s'].getItem(index))\n",
    "df_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- concat\n",
    "- concat_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## sample, 返回一个DataFrame, 包含源DataFrame 指定比例行数的数据\n",
    "df_sample = df_customers.sample(\n",
    "    withReplacement= False, fraction =0.2, seed = 1)\n",
    "df_sample.show()\n",
    "\n",
    "## sampleBy 按指定列，分层无放回抽样\n",
    "df_sample2 = df_sales.sampleBy(\n",
    "    'product', fractions= {\"iPhone\": 0.5, \"S6\": 0.5}, seed = 1)\n",
    "df_sample2.show()\n",
    "\n",
    "## randomSplit: 把DataFrame分割成多个DataFrame\n",
    "df_splits = df_customers.randomSplit([0.6,0.2,0.2])\n",
    "df_splits[0].show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "fr",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
