{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh-cn"
   },
   "source": [
    "`Word2Vec` 训练一个词向量模型 `Map(String, Vector)`，即将一个自然语言转换成数值向量，用于进一步的自然语言处理或机器学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class pyspark.ml.feature.Word2Vec(\n",
    "    *, vectorSize=100, minCount=5, numPartitions=1, \n",
    "    stepSize=0.025, maxIter=1, seed=None, inputCol=None, \n",
    "    outputCol=None, windowSize=5, maxSentenceLength=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Movie_Name_EN</th>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Crawl_Date</th>\n",
       "      <th>Number</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Star</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>然潘</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>2</td>\n",
       "      <td>更深的白色</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>2</td>\n",
       "      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>3</td>\n",
       "      <td>有意识的贱民</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>2</td>\n",
       "      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID           Movie_Name_EN Movie_Name_CN  Crawl_Date Number Username  \\\n",
       "0  0  Avengers Age of Ultron        复仇者联盟2  2017-01-22      1       然潘   \n",
       "1  1  Avengers Age of Ultron        复仇者联盟2  2017-01-22      2    更深的白色   \n",
       "2  2  Avengers Age of Ultron        复仇者联盟2  2017-01-22      3   有意识的贱民   \n",
       "\n",
       "         Date Star                                            Comment  Like  \n",
       "0  2015-05-13    3                                      连奥创都知道整容要去韩国。  2404  \n",
       "1  2015-04-24    2   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231  \n",
       "2  2015-04-26    2   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...  1052  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 kaggle 豆瓣评价数据\n",
    "df = spark.read.csv(f'C:/Users/chensy/Downloads/archive/DMSC.csv', header=True)\n",
    "# 剔除缺失数据\n",
    "df = df.dropna().limit(10)\n",
    "# 样本记录数\n",
    "print(f\"data length: {df.count() / 1e4:.2f}\")\n",
    "# 样本示例\n",
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们用到了 `jieba` 分词工具，把分词封装为一个 `udf` 对上面的 `DataFrame` 的 `Comment` 进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def jiebaCut(x):\n",
    "    return [w for w in jieba.cut(x) if len(w)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Movie_Name_EN: string (nullable = true)\n",
      " |-- Movie_Name_CN: string (nullable = true)\n",
      " |-- Crawl_Date: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Star: string (nullable = true)\n",
      " |-- Comment: string (nullable = true)\n",
      " |-- Like: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对评论进行分词处理\n",
    "df = df.withColumn('words', jiebaCut(df['Comment']))\n",
    "# 数据结构\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|                        words|\n",
      "+-----------------------------+\n",
      "|     [奥创, 知道, 整容, 韩国]|\n",
      "| [非常, 失望, 剧本, 完全, ...|\n",
      "|   [2015, 年度, 失望, 作品...|\n",
      "| [铁人, 勾引, 钢铁, 妇联, ...|\n",
      "| [虽然, 从头, 打到, 但是, ...|\n",
      "|[剧情, 不如, 第一集, 好玩,...|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看分词\n",
    "df.select('words').limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练词向量以及保存与读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `minCount`: 表示输入词在输入语料中至少出现多少次，才会进行向量转化，少于该出现次数的次将会在输入值中直接丢弃。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    vectorSize=8, minCount=2, numPartitions=4, maxIter=16,\n",
    "    seed=42, inputCol='words', outputCol='vecs').fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的数量：25\n",
      "+------+--------------------+\n",
      "|  word|              vector|\n",
      "+------+--------------------+\n",
      "|  角色|[0.01273861806839...|\n",
      "|  剧本|[-0.2398648262023...|\n",
      "|没什么|[-0.1424400210380...|\n",
      "|  彩蛋|[-0.0463520660996...|\n",
      "|  奥创|[-0.0279459767043...|\n",
      "|  实则|[0.30182662606239...|\n",
      "|  妇联|[0.08936640620231...|\n",
      "|  就是|[-0.1287378519773...|\n",
      "|  看过|[-0.2291299253702...|\n",
      "|  可以|[-0.3102417588233...|\n",
      "|  虽然|[-0.1917934864759...|\n",
      "|  失望|[-0.0342839509248...|\n",
      "|  只有|[-0.1234440803527...|\n",
      "|  打斗|[-0.0431661494076...|\n",
      "|  团结|[-0.2418100088834...|\n",
      "|  以为|[0.15235844254493...|\n",
      "|  剧情|[-0.0541103743016...|\n",
      "|  场面|[-0.0237559825181...|\n",
      "|  勾引|[0.45644679665565...|\n",
      "|  没有|[0.20933732390403...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 词向量的数量\n",
    "print(f'词向量的数量：{model.getVectors().count()}')\n",
    "# 展示词向量\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 保存词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 ToFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "model_path = \"./temp_data/word2vec\"\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 ToCsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.write.csv()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 ToHive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 保存向量\n",
    "df_vec = model.getVectors()\n",
    "df_vec.write.saveAsTable('db_name.table_name')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 读取词向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  word|              vector|\n",
      "+------+--------------------+\n",
      "|  角色|[0.01273861806839...|\n",
      "|  剧本|[-0.2398648262023...|\n",
      "|没什么|[-0.1424400210380...|\n",
      "|  彩蛋|[-0.0463520660996...|\n",
      "|  奥创|[-0.0279459767043...|\n",
      "|  实则|[0.30182662606239...|\n",
      "|  妇联|[0.08936640620231...|\n",
      "|  就是|[-0.1287378519773...|\n",
      "|  看过|[-0.2291299253702...|\n",
      "|  可以|[-0.3102417588233...|\n",
      "|  虽然|[-0.1917934864759...|\n",
      "|  失望|[-0.0342839509248...|\n",
      "|  只有|[-0.1234440803527...|\n",
      "|  打斗|[-0.0431661494076...|\n",
      "|  团结|[-0.2418100088834...|\n",
      "|  以为|[0.15235844254493...|\n",
      "|  剧情|[-0.0541103743016...|\n",
      "|  场面|[-0.0237559825181...|\n",
      "|  勾引|[0.45644679665565...|\n",
      "|  没有|[0.20933732390403...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取模型\n",
    "model_load = Word2VecModel.load(model_path)\n",
    "# 展示词向量\n",
    "model_load.getVectors().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 其他`Word2Vec`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取或设置最大\n",
    "word2Vec.setMaxIter(10)\n",
    "word2Vec.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除参数\n",
    "word2Vec.clear(word2Vec.maxIter)\n",
    "word2Vec.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置最小出现次数，设置输出列\n",
    "model.getMinCount()\n",
    "model.setInputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 词向量的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 词向量映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对映射后的向量序列直接取了平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| ID|                vecs|\n",
      "+---+--------------------+\n",
      "|  0|[-0.0069864941760...|\n",
      "|  1|[-0.0514680834859...|\n",
      "|  2|[0.05959105768646...|\n",
      "|  3|[0.04881695906321...|\n",
      "|  4|[-0.0299268453381...|\n",
      "|  5|[-0.0046156922785...|\n",
      "|  6|[-0.0051825046784...|\n",
      "|  7|[0.0,0.0,0.0,0.0,...|\n",
      "|  8|[-0.0096317072483...|\n",
      "|  9|[-0.0082069622479...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trans = model.transform(df)\n",
    "df_trans.select('ID', 'vecs').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 自定义映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "# 向量长度\n",
    "vec_length = model.getVectorSize()\n",
    "\n",
    "df_words = df.select('ID', F.explode('Words').alias('word'))\n",
    "df_words_cnt = df_words.groupBy('ID').count()\n",
    "df_vec = model.getVectors()\n",
    "df_words = df_words.join(df_vec, on='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ F.udf(returnType=ArrayType(FloatType()))\n",
    "def sparse_to_array(v):\n",
    "    \"\"\"将 Vector 转化为 array\"\"\"\n",
    "    v = DenseVector(v)\n",
    "    new_array = list([float(x) for x in v])\n",
    "    return new_array\n",
    "\n",
    "df_words = df_words.withColumn('vector_array', sparse_to_array('vector'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| ID|            features|\n",
      "+---+--------------------+\n",
      "|  0|[-0.0069864941760...|\n",
      "|  1|[-0.0514680834859...|\n",
      "|  2|[0.05959105768646...|\n",
      "|  3|[0.04881695906321...|\n",
      "|  4|[-0.0299268453381...|\n",
      "|  5|[-0.0046156922785...|\n",
      "|  6|[-0.0051825046784...|\n",
      "|  8|[-0.0096317072483...|\n",
      "|  9|[-0.0082069622479...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trans = df_words.groupBy('ID').agg(\n",
    "    *[F.sum(F.col('vector_array')[i]).alias(f'f_{i}') for i in range(vec_length)],)\n",
    "df_trans = df_trans.join(df_words_cnt, on='ID')\n",
    "\n",
    "for i in range(vec_length):\n",
    "    df_trans = df_trans.withColumn(f'f_{i}', F.col(f'f_{i}') / F.col(\"count\"))\n",
    "\n",
    "feature_names = [f'f_{i}' for i in range(vec_length)]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# 集合所有特征，放在features列里\n",
    "vec_assmebler = VectorAssembler(\n",
    "    inputCols=feature_names,\n",
    "    outputCol='features')\n",
    "\n",
    "# 对 df 进行合并特征操作\n",
    "df_features = vec_assmebler.transform(df_trans).select('ID', 'features').orderBy('ID')\n",
    "\n",
    "# word2vec 指标展示\n",
    "df_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 词向量应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 依据词向量找到相似的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh-cn"
   },
   "source": [
    "找到与`word`最相似的`num`个单词。`word` 可以是字符串或向量表示。\n",
    "返回单词与余弦相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('以为', 0.7477603554725647), ('实则', 0.6977234482765198)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.findSynonymsArray(\"失望\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|word|        similarity|\n",
      "+----+------------------+\n",
      "|以为|0.7477603554725647|\n",
      "|实则|0.6977234482765198|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"失望\", 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 计算全部词的余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+\n",
      "|id1|               vecs1|id2|               vecs2|\n",
      "+---+--------------------+---+--------------------+\n",
      "|  0|[-0.0069864941760...|  0|[-0.0069864941760...|\n",
      "|  0|[-0.0069864941760...|  1|[-0.0514680834859...|\n",
      "|  0|[-0.0069864941760...|  2|[0.05959105768646...|\n",
      "|  0|[-0.0069864941760...|  3|[0.04881695906321...|\n",
      "|  0|[-0.0069864941760...|  4|[-0.0299268453381...|\n",
      "|  0|[-0.0069864941760...|  5|[-0.0046156922785...|\n",
      "|  0|[-0.0069864941760...|  6|[-0.0051825046784...|\n",
      "|  0|[-0.0069864941760...|  7|[0.0,0.0,0.0,0.0,...|\n",
      "|  0|[-0.0069864941760...|  8|[-0.0096317072483...|\n",
      "|  0|[-0.0069864941760...|  9|[-0.0082069622479...|\n",
      "|  1|[-0.0514680834859...|  0|[-0.0069864941760...|\n",
      "|  1|[-0.0514680834859...|  1|[-0.0514680834859...|\n",
      "|  1|[-0.0514680834859...|  2|[0.05959105768646...|\n",
      "|  1|[-0.0514680834859...|  3|[0.04881695906321...|\n",
      "|  1|[-0.0514680834859...|  4|[-0.0299268453381...|\n",
      "|  1|[-0.0514680834859...|  5|[-0.0046156922785...|\n",
      "|  1|[-0.0514680834859...|  6|[-0.0051825046784...|\n",
      "|  1|[-0.0514680834859...|  7|[0.0,0.0,0.0,0.0,...|\n",
      "|  1|[-0.0514680834859...|  8|[-0.0096317072483...|\n",
      "|  1|[-0.0514680834859...|  9|[-0.0082069622479...|\n",
      "+---+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trans = model.transform(df)\n",
    "df_trans.select('ID', 'vecs')\n",
    "\n",
    "df_cross = df_trans.select(\n",
    "    F.col('id').alias('id1'),\n",
    "    F.col('vecs').alias('vecs1'))\\\n",
    "    .crossJoin(\n",
    "        df_trans.select(\n",
    "            F.col('id').alias('id2'),\n",
    "            F.col('vecs').alias('vecs2')))\n",
    "df_cross.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+-----------+\n",
      "|id1|               vecs1|id2|               vecs2|        sim|\n",
      "+---+--------------------+---+--------------------+-----------+\n",
      "|  0|[-0.0069864941760...|  0|[-0.0069864941760...|        1.0|\n",
      "|  0|[-0.0069864941760...|  1|[-0.0514680834859...| -0.3717363|\n",
      "|  0|[-0.0069864941760...|  2|[0.05959105768646...|  0.3434972|\n",
      "|  0|[-0.0069864941760...|  3|[0.04881695906321...| 0.25583833|\n",
      "|  0|[-0.0069864941760...|  4|[-0.0299268453381...| -0.3790742|\n",
      "|  0|[-0.0069864941760...|  5|[-0.0046156922785...|0.052120406|\n",
      "|  0|[-0.0069864941760...|  6|[-0.0051825046784...| -0.3678641|\n",
      "|  0|[-0.0069864941760...|  7|[0.0,0.0,0.0,0.0,...|        1.0|\n",
      "|  0|[-0.0069864941760...|  8|[-0.0096317072483...| 0.55050826|\n",
      "|  0|[-0.0069864941760...|  9|[-0.0082069622479...|-0.32809532|\n",
      "|  1|[-0.0514680834859...|  0|[-0.0069864941760...| -0.3717363|\n",
      "|  1|[-0.0514680834859...|  1|[-0.0514680834859...|        1.0|\n",
      "|  1|[-0.0514680834859...|  2|[0.05959105768646...|-0.91381425|\n",
      "|  1|[-0.0514680834859...|  3|[0.04881695906321...| -0.6825831|\n",
      "|  1|[-0.0514680834859...|  4|[-0.0299268453381...|  0.9800788|\n",
      "|  1|[-0.0514680834859...|  5|[-0.0046156922785...|-0.42568147|\n",
      "|  1|[-0.0514680834859...|  6|[-0.0051825046784...|  0.8500338|\n",
      "|  1|[-0.0514680834859...|  7|[0.0,0.0,0.0,0.0,...|        1.0|\n",
      "|  1|[-0.0514680834859...|  8|[-0.0096317072483...| 0.12655479|\n",
      "|  1|[-0.0514680834859...|  9|[-0.0082069622479...|  0.7709821|\n",
      "+---+--------------------+---+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用 cosine 构建相似度计算 udf\n",
    "from scipy import spatial\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "def sim(x, y):\n",
    "    return float(1 - spatial.distance.cosine(x, y))\n",
    "\n",
    "# 计算两个向量间的相似度 sim\n",
    "df_cross = df_cross.withColumn('sim', sim(df_cross['vecs1'], df_cross['vecs2']))\n",
    "df_cross.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 停词剔除 `StopWordsRemover`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class pyspark.ml.feature.StopWordsRemover(\n",
    "    *, inputCol=None, outputCol=None, stopWords=None, \n",
    "    caseSensitive=False, locale=None, inputCols=None, outputCols=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh-cn"
   },
   "source": [
    "从输入中过滤掉停用词的特征转换器。从 `3.0.0` 开始，\n",
    "`StopWordsRemover` 可以通过设置 `inputCols` 参数一次过滤掉多个列。\n",
    "请注意，当同时设置 `inputCol` 和 `inputCols` 参数时，将抛出异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 停词剔除示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     text|words|\n",
      "+---------+-----+\n",
      "|[a, b, c]|  [a]|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n",
    "remover = StopWordsRemover(\n",
    "    stopWords=[\"b\", 'C'], caseSensitive=False,\n",
    "    inputCol='text', outputCol='words')\n",
    "\n",
    "# 剔除停词\n",
    "remover.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 停词方法参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'C']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取停词参数\n",
    "display(remover.getStopWords())\n",
    "\n",
    "# 是否区分大小写，默认不区分大小写\n",
    "display(remover.getCaseSensitive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [数据来源](https://www.kaggle.com/utmhikari/doubanmovieshortcomments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "zh-cn",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh-cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
