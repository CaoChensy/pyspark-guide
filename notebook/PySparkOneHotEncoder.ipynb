{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "‘’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43798e65",
   "metadata": {},
   "source": [
    "# OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361566c",
   "metadata": {},
   "source": [
    "```python\n",
    "class pyspark.ml.feature.OneHotEncoder(\n",
    "    *, inputCols=None, outputCols=None, handleInvalid='error',\n",
    "    dropLast=True, inputCol=None, outputCol=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936d49b",
   "metadata": {},
   "source": [
    "A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\n",
    "\n",
    "When handleInvalid is configured to ‘keep’, an extra “category” indicating invalid values is added as last category. So when dropLast is true, invalid values are encoded as all-zeros vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e102728",
   "metadata": {},
   "source": [
    "StringIndexer\n",
    "for converting categorical values into category indices\n",
    "\n",
    "Notes\n",
    "\n",
    "This is different from scikit-learn’s OneHotEncoder, which keeps all categories. The output vectors are sparse.\n",
    "\n",
    "When encoding multi-column by using inputCols and outputCols params, input/output cols come in pairs, specified by the order in the arrays, and each pair is treated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b48ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "052f861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(0.0,), (1.0,), (2.0,)], [\"input\"])\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols=['input'],\n",
    "    outputCols=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65d1218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ohe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22b1b9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef1e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: double (nullable = true)\n",
      " |-- output: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f51873b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|input|       output|\n",
      "+-----+-------------+\n",
      "|  0.0|(2,[0],[1.0])|\n",
      "|  1.0|(2,[1],[1.0])|\n",
      "|  2.0|    (2,[],[])|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7880c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(input=0.0, output=SparseVector(2, {0: 1.0})),\n",
       " Row(input=1.0, output=SparseVector(2, {1: 1.0})),\n",
       " Row(input=2.0, output=SparseVector(2, {}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setOutputCols([\"output\"])\n",
    "\n",
    "model.getHandleInvalid()\n",
    "\n",
    "model.transform(df).head().output\n",
    "\n",
    "single_col_ohe = OneHotEncoder(inputCol=\"input\", outputCol=\"output\")\n",
    "single_col_model = single_col_ohe.fit(df)\n",
    "single_col_model.transform(df).head().output\n",
    "\n",
    "ohePath = temp_path + \"/ohe\"\n",
    "ohe.save(ohePath)\n",
    "loadedOHE = OneHotEncoder.load(ohePath)\n",
    "loadedOHE.getInputCols() == ohe.getInputCols()\n",
    "\n",
    "modelPath = temp_path + \"/ohe-model\"\n",
    "model.save(modelPath)\n",
    "loadedModel = OneHotEncoderModel.load(modelPath)\n",
    "loadedModel.categorySizes == model.categorySizes\n",
    "\n",
    "loadedModel.transform(df).take(1) == model.transform(df).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c50799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eceb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a1642e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "| id|value|valueIndex|valueIndexVec|\n",
      "+---+-----+----------+-------------+\n",
      "|  0|    a|       0.0|(3,[0],[1.0])|\n",
      "|  1|    b|       2.0|(3,[2],[1.0])|\n",
      "|  2|    c|       1.0|(3,[1],[1.0])|\n",
      "|  3|    a|       0.0|(3,[0],[1.0])|\n",
      "|  4|    a|       0.0|(3,[0],[1.0])|\n",
      "|  5|    c|       1.0|(3,[1],[1.0])|\n",
      "|  6|    d|       3.0|    (3,[],[])|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\"),\n",
    "    (6, \"d\"),\n",
    "], [\"id\", \"value\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"value\", outputCol=\"valueIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"valueIndex\", outputCol=\"valueIndexVec\")\\\n",
    "    .fit(indexed)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19ec46",
   "metadata": {},
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30c1f2",
   "metadata": {},
   "source": [
    "```python\n",
    "class pyspark.ml.feature.StringIndexer(\n",
    "    *, inputCol=None, outputCol=None, inputCols=None, \n",
    "    outputCols=None, handleInvalid='error', \n",
    "    stringOrderType='frequencyDesc')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76140f6f",
   "metadata": {},
   "source": [
    "A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e405441",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(\n",
    "    inputCol=\"label\", outputCol=\"indexed\",\n",
    "    stringOrderType=\"frequencyDesc\",\n",
    "    handleInvalid='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b85582",
   "metadata": {},
   "source": [
    "handleInvalid = Param(parent='undefined', name='handleInvalid', doc=\"how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels).\")¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e44ce",
   "metadata": {},
   "source": [
    "stringOrderType = Param(parent='undefined', name='stringOrderType', doc='How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. Default is frequencyDesc. In case of equal frequency when under frequencyDesc/Asc, the strings are further sorted alphabetically')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = model.transform(stringIndDf)\n",
    "sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n",
    "    key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94dfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverter = IndexToString(inputCol=\"indexed\", outputCol=\"label2\", labels=model.labels)\n",
    "itd = inverter.transform(td)\n",
    "sorted(set([(i[0], str(i[1])) for i in itd.select(itd.id, itd.label2).collect()]),\n",
    "    key=lambda x: x[0])\n",
    "\n",
    "stringIndexerPath = temp_path + \"/string-indexer\"\n",
    "stringIndexer.save(stringIndexerPath)\n",
    "loadedIndexer = StringIndexer.load(stringIndexerPath)\n",
    "loadedIndexer.getHandleInvalid() == stringIndexer.getHandleInvalid()\n",
    "\n",
    "modelPath = temp_path + \"/string-indexer-model\"\n",
    "model.save(modelPath)\n",
    "loadedModel = StringIndexerModel.load(modelPath)\n",
    "loadedModel.labels == model.labels\n",
    "\n",
    "indexToStringPath = temp_path + \"/index-to-string\"\n",
    "inverter.save(indexToStringPath)\n",
    "loadedInverter = IndexToString.load(indexToStringPath)\n",
    "loadedInverter.getLabels() == inverter.getLabels()\n",
    "\n",
    "loadedModel.transform(stringIndDf).take(1) == model.transform(stringIndDf).take(1)\n",
    "\n",
    "stringIndexer.getStringOrderType()\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\",\n",
    "    stringOrderType=\"alphabetDesc\")\n",
    "model = stringIndexer.fit(stringIndDf)\n",
    "td = model.transform(stringIndDf)\n",
    "sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n",
    "    key=lambda x: x[0])\n",
    "\n",
    "fromlabelsModel = StringIndexerModel.from_labels([\"a\", \"b\", \"c\"],\n",
    "    inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\")\n",
    "result = fromlabelsModel.transform(stringIndDf)\n",
    "sorted(set([(i[0], i[1]) for i in result.select(result.id, result.indexed).collect()]),\n",
    "    key=lambda x: x[0])\n",
    "\n",
    "testData = sc.parallelize([Row(id=0, label1=\"a\", label2=\"e\"),\n",
    "                           Row(id=1, label1=\"b\", label2=\"f\"),\n",
    "                           Row(id=2, label1=\"c\", label2=\"e\"),\n",
    "                           Row(id=3, label1=\"a\", label2=\"f\"),\n",
    "                           Row(id=4, label1=\"a\", label2=\"f\"),\n",
    "                           Row(id=5, label1=\"c\", label2=\"f\")], 3)\n",
    "multiRowDf = spark.createDataFrame(testData)\n",
    "inputs = [\"label1\", \"label2\"]\n",
    "outputs = [\"index1\", \"index2\"]\n",
    "stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n",
    "model = stringIndexer.fit(multiRowDf)\n",
    "result = model.transform(multiRowDf)\n",
    "sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n",
    "    result.index2).collect()]), key=lambda x: x[0])\n",
    "\n",
    "fromlabelsModel = StringIndexerModel.from_arrays_of_labels([[\"a\", \"b\", \"c\"], [\"e\", \"f\"]],\n",
    "    inputCols=inputs, outputCols=outputs)\n",
    "result = fromlabelsModel.transform(multiRowDf)\n",
    "sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n",
    "    result.index2).collect()]), key=lambda x: x[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
