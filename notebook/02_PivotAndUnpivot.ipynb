{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6763518a",
   "metadata": {},
   "source": [
    "## 数据透视表`Pivot`与逆透视表`Unpivot`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1528942",
   "metadata": {},
   "source": [
    "**数据分析中常用到数据宽表，也会存在数据长表，本节主要尝试数据长表与数据宽表的相互转换，以及阐述这样转换的一些好处。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ac63",
   "metadata": {},
   "source": [
    "`Spark pivot()` 函数用于将数据从一个 `DataFrame` 行旋转为多列，而 `unpivot` 用于将列转换为行。\n",
    "\n",
    "在本文中，将解释如何使用 `pivot()` 函数将一行或多行转置为列。\n",
    "\n",
    "`Pivot()` 是一种聚合，其中一个分组列值被转换为具有不同数据的单个列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35962347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a00445",
   "metadata": {},
   "source": [
    "## 创建数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2248159",
   "metadata": {},
   "source": [
    "原始数据是一个数据长表。\n",
    "DataFrame `df` 包含 3 列 `Product`、`Amount` 和 `Country`，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f28ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = (\n",
    "    (\"Banana\", 1000, \"USA\"), \n",
    "    (\"Carrots\", 1500, \"USA\"), \n",
    "    (\"Beans\", 1600, \"USA\"),\n",
    "    (\"Orange\", 2000, \"USA\"),\n",
    "    (\"Orange\", 2000, \"USA\"),\n",
    "    (\"Banana\", 400, \"China\"),\n",
    "    (\"Carrots\", 1200, \"China\"),\n",
    "    (\"Beans\", 1500, \"China\"),\n",
    "    (\"Orange\", 4000, \"China\"),\n",
    "    (\"Banana\", 2000, \"Canada\"),\n",
    "    (\"Carrots\", 2000, \"Canada\"),\n",
    "    (\"Beans\", 2000, \"Mexico\"))\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Product\", \"Amount\", \"Country\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429cca1",
   "metadata": {},
   "source": [
    "## `Pivot` 数据透视表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce74db3",
   "metadata": {},
   "source": [
    "**现需要将数据长表转换为数据宽表，便于对每个产品的数据进行分析。**\n",
    "\n",
    "`Spark SQL` 提供`pivot()`将数据从一列旋转到多列（行到列）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c707207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-------+------+\n",
      "|Country|Banana|Beans|Carrots|Orange|\n",
      "+-------+------+-----+-------+------+\n",
      "|  China|   400| 1500|   1200|  4000|\n",
      "|    USA|  1000| 1600|   1500|  2000|\n",
      "| Mexico|  null| 2000|   null|  null|\n",
      "| Canada|  2000| null|   2000|  null|\n",
      "+-------+------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Country').pivot('Product').agg(\n",
    "    F.first(\"Amount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfd580",
   "metadata": {},
   "source": [
    "如果数据不存在，默认情况下它表示为空。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656e1cb",
   "metadata": {},
   "source": [
    "`Spark 2.0` 以后的性能在 `Pivot` 上得到了改进，\n",
    "但是，如果您使用的是较低版本，出于性能的考虑，\n",
    "建议提供列数据作为函数的参数，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d0fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|Country|Banana|Beans|\n",
      "+-------+------+-----+\n",
      "|  China|   400| 1500|\n",
      "|    USA|  1000| 1600|\n",
      "| Mexico|  null| 2000|\n",
      "| Canada|  2000| null|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Country').pivot('Product', ['Banana', 'Beans']).agg(\n",
    "    F.first(\"Amount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a630227",
   "metadata": {},
   "source": [
    "## `Unpivot` 逆透视"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72746010",
   "metadata": {},
   "source": [
    "**现将数据宽表再转换为数据长表。**\n",
    "\n",
    "`Unpivo`t 是一个反向操作，我们可以通过将列值旋转为行值。\n",
    "`Spark SQL` 没有 `unpivot` 功能，因此将使用该`stack()`功能。\n",
    "下面的代码将列产品转换为行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71eaf1e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T07:09:54.952988Z",
     "start_time": "2021-03-16T07:09:47.731791Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pivot = df.groupBy('Country').pivot('Product').agg(\n",
    "    F.first(\"Amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232da85",
   "metadata": {},
   "source": [
    "### 写法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5475d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|Country|Product|Amount|\n",
      "+-------+-------+------+\n",
      "|  China| Banana|   400|\n",
      "|  China|  Beans|  1500|\n",
      "|  China|Carrots|  1200|\n",
      "|  China| Orange|  4000|\n",
      "|    USA| Banana|  1000|\n",
      "|    USA|  Beans|  1600|\n",
      "|    USA|Carrots|  1500|\n",
      "|    USA| Orange|  2000|\n",
      "| Mexico| Banana|  null|\n",
      "| Mexico|  Beans|  2000|\n",
      "| Mexico|Carrots|  null|\n",
      "| Mexico| Orange|  null|\n",
      "| Canada| Banana|  2000|\n",
      "| Canada|  Beans|  null|\n",
      "| Canada|Carrots|  2000|\n",
      "| Canada| Orange|  null|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.selectExpr(\n",
    "    \"`Country`\", \"stack(4, 'Banana', `Banana`, 'Beans', `Beans`, 'Carrots', `Carrots`, 'Orange', `Orange`) as (`Product`,`Amount`)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e0afe",
   "metadata": {},
   "source": [
    "### 写法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba28fcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|Country|Product|Amount|\n",
      "+-------+-------+------+\n",
      "|  China| Banana|   400|\n",
      "|  China|  Beans|  1500|\n",
      "|  China|Carrots|  1200|\n",
      "|  China| Orange|  4000|\n",
      "|    USA| Banana|  1000|\n",
      "|    USA|  Beans|  1600|\n",
      "|    USA|Carrots|  1500|\n",
      "|    USA| Orange|  2000|\n",
      "| Mexico| Banana|  null|\n",
      "| Mexico|  Beans|  2000|\n",
      "| Mexico|Carrots|  null|\n",
      "| Mexico| Orange|  null|\n",
      "| Canada| Banana|  2000|\n",
      "| Canada|  Beans|  null|\n",
      "| Canada|Carrots|  2000|\n",
      "| Canada| Orange|  null|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.select(\n",
    "    \"`Country`\", \n",
    "    F.expr(\"stack(4, 'Banana', `Banana`, 'Beans', `Beans`, 'Carrots', `Carrots`, 'Orange', `Orange`) as (`Product`,`Amount`)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50533126",
   "metadata": {},
   "source": [
    "## 将`stack`封装成函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015f10e",
   "metadata": {},
   "source": [
    "> 将以上逆透视过程封装成函数，方便后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788c2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot(df, columns, val_type=None, index_name='uuid', feature_name='name', feature_value='value'):\n",
    "    \"\"\"\n",
    "    描述：对数据表进行反pivot操作\n",
    "    \n",
    "    :param df[DataFrame]:                 pyspark dataframe\n",
    "    :param columns[List]:                 需要转换的列\n",
    "    :param val_type[pyspark.sql.types]:   数据类型\n",
    "    :param index_name[String]:            index column\n",
    "    :param feature_name[String]:          特征列\n",
    "    :param feature_value[String]:         数值列\n",
    "    \"\"\"\n",
    "    if val_type is not None:\n",
    "        df = df.select(index_name, *[F.col(col).cast(val_type()) for col in columns])\n",
    "    \n",
    "    stack_query = []\n",
    "    for col in columns:\n",
    "        stack_query.append(f\"'{col}', `{col}`\")\n",
    "\n",
    "    df = df.selectExpr(\n",
    "        f\"`{index_name}`\", f\"stack({len(stack_query)}, {', '.join(stack_query)}) as (`{feature_name}`, `{feature_value}`)\"\n",
    "    ).orderBy(index_name, feature_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a0ebf",
   "metadata": {},
   "source": [
    "### 应用以上函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e212349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|Country|Product|Amount|\n",
      "+-------+-------+------+\n",
      "| Canada| Banana|2000.0|\n",
      "| Canada|  Beans|  null|\n",
      "| Canada|Carrots|2000.0|\n",
      "| Canada| Orange|  null|\n",
      "|  China| Banana| 400.0|\n",
      "|  China|  Beans|1500.0|\n",
      "|  China|Carrots|1200.0|\n",
      "|  China| Orange|4000.0|\n",
      "| Mexico| Banana|  null|\n",
      "| Mexico|  Beans|2000.0|\n",
      "| Mexico|Carrots|  null|\n",
      "| Mexico| Orange|  null|\n",
      "|    USA| Banana|1000.0|\n",
      "|    USA|  Beans|1600.0|\n",
      "|    USA|Carrots|1500.0|\n",
      "|    USA| Orange|2000.0|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df_unpivot = unpivot(\n",
    "    df_pivot, columns=['Banana', 'Beans', 'Carrots', 'Orange'], val_type=FloatType,\n",
    "    index_name='Country', feature_name='Product', feature_value='Amount')\n",
    "df_unpivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a76892",
   "metadata": {},
   "source": [
    "### 长表统计分析的好处"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7bb84",
   "metadata": {},
   "source": [
    "**在长表状态下便于进行数据的分组聚合操作。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a23800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+---------+\n",
      "|Country|ProductCount|AmountMean|AmountMax|\n",
      "+-------+------------+----------+---------+\n",
      "|  China|           4|    1775.0|   4000.0|\n",
      "|    USA|           4|    1525.0|   2000.0|\n",
      "| Mexico|           4|    2000.0|   2000.0|\n",
      "| Canada|           4|    2000.0|   2000.0|\n",
      "+-------+------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unpivot.groupBy('Country').agg(\n",
    "    F.countDistinct('Product').alias('ProductCount'),\n",
    "    F.mean('Amount').alias('AmountMean'),\n",
    "    F.max('Amount').alias('AmountMax'),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882d8db",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
